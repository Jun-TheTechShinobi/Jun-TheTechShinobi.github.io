<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Cheapskate&#39;s Homebrew AI Lab | Tech Shinobi</title>
<meta name="keywords" content="Vostro, Cheapskate, Optiplex, Opensource, Philosophy, API, Richard M. Stallma, Debian, NVIDIA Tesla M40, swap, Hardware, Thermal Mod, BIOS, Subs AI Whisper, Text generation web UI, Llama2 uncensored, TTS Generation WebUI, Voice Clone, h2oGPT, chatpdf, Chinese LLMs, Artificial Imbecility">
<meta name="description" content="Old Stories The computer hardware used to be more playful and worth tinkering. My favorate platfom from a decade ago, Sandy Bridge on LGA 1155, can still be powerful even today.
Back in the days, I used to repair people&rsquo;s electronics for free. Because of that, I also received a lots of spares and e-waste in exchange. One of the best was a LGA 1155 motherboard with i5-2300 on it. Then, I bought a cheap E3-1245 and GTX 750 Ti to make it a gaming rig.">
<meta name="author" content="Jun">
<link rel="canonical" href="https://techshinobi.org/posts/cheapai/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css" integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.e85ad0406048e8176e1c7661b25d5c69297ddfe41dc4124cf75ecb99a4f7b3d1.js" integrity="sha256-6FrQQGBI6BduHHZhsl1caSl93&#43;QdxBJM917LmaT3s9E="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://techshinobi.org/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://techshinobi.org/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://techshinobi.org/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://techshinobi.org/apple-touch-icon.png">
<link rel="mask-icon" href="https://techshinobi.org/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Cheapskate&#39;s Homebrew AI Lab" />
<meta property="og:description" content="Old Stories The computer hardware used to be more playful and worth tinkering. My favorate platfom from a decade ago, Sandy Bridge on LGA 1155, can still be powerful even today.
Back in the days, I used to repair people&rsquo;s electronics for free. Because of that, I also received a lots of spares and e-waste in exchange. One of the best was a LGA 1155 motherboard with i5-2300 on it. Then, I bought a cheap E3-1245 and GTX 750 Ti to make it a gaming rig." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://techshinobi.org/posts/cheapai/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-23T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2023-10-23T00:00:00&#43;00:00" /><meta property="og:site_name" content="TechShinobi" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Cheapskate&#39;s Homebrew AI Lab"/>
<meta name="twitter:description" content="Old Stories The computer hardware used to be more playful and worth tinkering. My favorate platfom from a decade ago, Sandy Bridge on LGA 1155, can still be powerful even today.
Back in the days, I used to repair people&rsquo;s electronics for free. Because of that, I also received a lots of spares and e-waste in exchange. One of the best was a LGA 1155 motherboard with i5-2300 on it. Then, I bought a cheap E3-1245 and GTX 750 Ti to make it a gaming rig."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://techshinobi.org/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Cheapskate's Homebrew AI Lab",
      "item": "https://techshinobi.org/posts/cheapai/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Cheapskate's Homebrew AI Lab",
  "name": "Cheapskate\u0027s Homebrew AI Lab",
  "description": "Old Stories The computer hardware used to be more playful and worth tinkering. My favorate platfom from a decade ago, Sandy Bridge on LGA 1155, can still be powerful even today.\nBack in the days, I used to repair people\u0026rsquo;s electronics for free. Because of that, I also received a lots of spares and e-waste in exchange. One of the best was a LGA 1155 motherboard with i5-2300 on it. Then, I bought a cheap E3-1245 and GTX 750 Ti to make it a gaming rig.",
  "keywords": [
    "Vostro", "Cheapskate", "Optiplex", "Opensource", "Philosophy", "API", "Richard M. Stallma", "Debian", "NVIDIA Tesla M40", "swap", "Hardware", "Thermal Mod", "BIOS", "Subs AI Whisper", "Text generation web UI", "Llama2 uncensored", "TTS Generation WebUI", "Voice Clone", "h2oGPT", "chatpdf", "Chinese LLMs", "Artificial Imbecility"
  ],
  "articleBody": "Old Stories The computer hardware used to be more playful and worth tinkering. My favorate platfom from a decade ago, Sandy Bridge on LGA 1155, can still be powerful even today.\nBack in the days, I used to repair people’s electronics for free. Because of that, I also received a lots of spares and e-waste in exchange. One of the best was a LGA 1155 motherboard with i5-2300 on it. Then, I bought a cheap E3-1245 and GTX 750 Ti to make it a gaming rig. I played a lot of games on that, such as Dark Souls series and Metro series. Before it was sold, last games I played on this build was Metro Exodus and Elden Ring.\nSandy Bridge was the last generation using soldered integrated heat spreader (IHS) in a long time. It’s cooler and stays cool over time way better than its thermal-pasted successors (e.g. Haswell/Hotwell). Back then, there was a doggerel joking about AMD’s performance and Intel’s temperature, saying “Unlocking AMD, Delidding Intel.”\nThe real fun project was a LGA771 to LGA775 Mod on a Dell Vostro 220s. It was a hardware hack that letting this low-end Vostro uses a dirt cheap yet powerful Xeon. The motherboard also has 4 SATA ports so it works well as a NAS.\nNext is our today’s topic, Dell Optiplex 3010 SFF. It was on sell for quite a few months but seemingly no one wants it at all. I don’t think it’s a completely garbage comparing the Vostro above. It came with an i3-3220 but has been upgraded to the i5-2300 that was replaced from the gaming rig for the sake of LGA 1155. I’m not planning to put an Xeon E3 on it but a weird graphic card without output port.\nPhilosophy If you’re wondering—why bother spending such amount of time and effort tinkering those e-wastes? Just because of being a cheapskate and saving money?\nHere is the answer from my previous post:\n I really like the idea from cheapskatesguide and lowtechmagazine that could save people from the pitfalls of consumerism. Moreover, to me it’s not about saving money on technology nor adapting certain lifestyles. This is a manifestation of ideology, the way of pursuing freedom.\n By this chance, I would like to add more details on that.\n On the internet, proprietary software isn’t the only way to lose your freedom. Service as a Software Substitute, or SaaSS, is another way to let someone else have power over your computing.\nIf you use SaaSS, the server operator controls your computing. It requires entrusting all the pertinent data to the server operator, which will be forced to show it to the state as well—who does that server really serve, after all?\nSaaSS does not require covert code to obtain the user’s data. Instead, users must send their data to the server in order to use it. This has the same effect as spyware: the server operator gets the data—with no special effort, by the nature of SaaSS.\nWith SaaSS, the server operator can change the software in use on the server. He ought to be able to do this, since it’s his computer; but the result is the same as using a proprietary application program with a universal back door: someone has the power to silently impose changes in how the user’s computing gets done.\nThus, SaaSS is equivalent to running proprietary software with spyware and a universal back door. It gives the server operator unjust power over the user, and that power is something we must resist.\nSaaSS always subjects you to the power of the server operator, and the only remedy is, Don’t use SaaSS! Don’t use someone else’s server to do your own computing on data provided by you.\n These are selected from Free Software, Free Society and I recommend to read the whole book if you have not already.\nRMS was right, once again. SaaS is dangerous to human liberty and I was aware of it. Since it was called “the cloud”, I’ve been self-hosting and peer-to-peer everything back that time.\nNowadays, it’s called “Generative AI”. This is why the first chapter of my Stable Diffusion article called “No DALL-E, No Midjourney and No Colab” where talks about neutrality and transparency. AIaaS leaks data and not secure. Not only ChatGPT can become BadGPT, but also open-source LLM can become PoisonGPT. Just like Diffusion models can have malware. Never blindly trust something just because it’s open-source.\nHardware Note: This is rather a rough record than a proper guide. Keep in mind, be sure you have enough experience tinkering PC hardware. Proceed with caution and at your own risk.\nDuring the summer, I was working on my research paper and now I have some time back to this fun project.\nAlthough I had some fun and had done quite some projects with it, the biggest con about My mini Server is the VRAM capacity. 4GB is too limiting when I attempt to do training, like LoRA models. Not only that, even using ControlNet or running Open LLM are too restrained. So I decide to buy a NVIDIA Tesla M40, with 24 GB of VRAM which is the largest amount I can get from a cheap single card.\nMoney on the parts ($180-250 in total) :\n Dell Optiplex (Sandy Bridge), $0  In my case it’s 3010 SFF but other models (7010/9010) would be better On Ebay $40 for whole unit, $20 for motherboard only These models apears in surplus or thrift store quite often With the BIOS hack, neither Above 4G Decoding, Resizable BAR nor pci=realloc are needed   NVIDIA Tesla M40 24 GB, $110 Corsair AIO Cooler, $25  It’s the cheapest I can find, unknown model The condition is working but looks pretty much AS-IS I refilled it with purified water, checked the seal and pressure tested the pump No mounting bracket and I don’t need it either The screw holes on M40 is 58 x 58 mm, the smaller cooler surface the better (need space to put small heatsinks on VRAM chips) A $17 NZXT Kraken G12 is the proper way to go, but those compatible coolers can be expensive even buying used Most CPU AIO coolers would do it though, if using zip tie method Regular CPU air coolers may be too heavy for our card, and the blower fan adapter method is loud, ineffecient yet not cheap   Seasonic SSR-550RM, $28  Just a little bit overkill but it’s a good deal   CPU 8 Pin EPS Cable for Seasonic Modular PSU, $10 Dual PSU Adapter, $9  Optional, bought it for convenience   1TB SATA SSD, $0  Optional, possible to hack in NVMe drives for larger form like 7010/9010    GPU Selection Depending on price, availability and capacity, only K80, M40 and P40 are in my options.\n7xx(Kepler)：Tesla K80 24G 9xx(Maxwell)：Tesla M40 12G/24G，Tesla M4 4G 10x0(Pascal)：Tesla P100 16G，Tesla P40 24G，Tesla P4 8G 20x0(Volta/Turing)：Tesla T4 16G，Tesla V100 16G/32G 30x0(Ampere)：NVIDIA A100 40/80GB，NVIDIA A40 48GB，NVIDIA RTX A6000 48G The best bang for the budget seems like the old good K80, but it isn’t. The driver for Kepler cards are stucked with 440.95.01 and CUDA Toolkit 11.4. Besides that, K80 24G version is actually two 12G versions so it may only recognize half of the VRAM in some application.\nThe problem with Pascal card is that costs more money but not worth it. The support of quantization matters when it comes to AI inference. However, in order to get the performance gain from utilizing quantization. We are not only need to have the supported hardware and use the correct model, but the software needs to support it as well. Eventhough P100 supports FP16 and P40 supports INT8, they are not well supported by the upstream.\nTherefore, if using INT8 or FP16 is not expected, because most of the time it runs with FP32, no reason to spend more on P100 and P40. M40 is the sweet spot without doubt, and it saves a lot of hassle on optimazation. All in all, M40 is great for non-production evironment where performance isn’t the priority.\nAbout Power Supply Because there is only a 4+4 pin CPU power cable on my 550w PSU and Tesla cards require CPU’s EPS connection. I bought a $7 spliting PCIe to EPS adapter. Sadly, it melt within minutes.\nIt’s very likely that the melting was caused by the copper in the cable was too thin or the resistance of the cable was too high. To power the 250w GPU, a thicker stronger cable is required.\nI think it’s possible to force the 4+4 pin male connector fit into the 8 pin female socket on the GPU by filing it, but didn’t try. Finally, I bought a dedicated modular cable instead of another adapter. It says “UL1007 18AWG tinned copper wire with high current terminal” that sounds strong enough for the task and it indeed does.\nThe dual PSU adapter is more convenient than a switch jumper cable and safer than a paper clip.\nThermal MOD (Zip-tie Method) When hardware is incompatible, zip tie is our best friend.\nI have talked a lot on the cooling system already. Here just a showcase for my zip tie method mounting any sized AIO cooler, case fan and heat sink.\nIt is quite stable and low-noise. Works on both CPU and GPU. As a bounus, the zip ties also hold up the GPU backplate quite well, and the water pipes can support the GPU to balance its weight if tweaked to a good angle. So, nothing will come loose or bend in a long run.\nIn my CASE, the cardboard box, the cooling is a bit overkill and I’m quite happy about 23-26C idle and 46-52C full-load temperature.\nVery cool temperature on idle:\n+---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.104.05 Driver Version: 535.104.05 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 Tesla M40 24GB On | 00000000:01:00.0 Off | 0 | | N/A 24C P8 18W / 250W | 0MiB / 23040MiB | 0% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | No running processes found | +---------------------------------------------------------------------------------------+ Full-load running Llama2:\n+---------------------------------------------------------------------------------------+ | NVIDIA-SMI 535.104.05 Driver Version: 535.104.05 CUDA Version: 12.2 | |-----------------------------------------+----------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+======================+======================| | 0 Tesla M40 24GB On | 00000000:01:00.0 Off | 0 | | N/A 52C P0 250W / 250W | 16777MiB / 23040MiB | 100% Default | | | | N/A | +-----------------------------------------+----------------------+----------------------+ +---------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=======================================================================================| | 0 N/A N/A 351272 C python3 16774MiB | +---------------------------------------------------------------------------------------+ Firmware The report of 7010 is very brief, so I decide to extend it a little bit based on my 3010 reimplementation.\nFirst off follow this NVME guide to dump the BIOS rom.\nOn my 3010, the jumper needs to remove CPU cooler(take the entire cooler with the fan) to find.\nAccording to Dell official 2-3 pin: normal default; 1-2 pin: clear ME(service mode) to move the jumper. It is actually MECLR1 on my board (right side of the photo).\nAfter updated my BIOS version from A09 to A20, I made a backup using fptw64.exe -d backup.bin.\nNext, follow this DSDT guide but during step 5, after finishing the changes, add some more extra changes from this issue:\n Change If (((MM64 == Zero) || (OSYS to If (((OSYS  Change ElseIf (E4GM) to Else Remove this section   Else { CreateDWordField (BUF0, \\_SB.PCI0._Y0F._LEN, M4LN) // _LEN: Length M4LN = Zero } Continue rest of the steps to finish the DSDT mod, then proceed with UEFI Patch guide to get the patched version of the mod rom.\nFinally, use fptw64.exe -bios -f modpatched.bin to flash the rom.\nFiles I have share all files generated during the process in this repo.\nA20MOD.rom.patched.bin is the binary file named modpatched.bin in the final step above. The souce code is in DSDTMod.dsl.\nSoftware Debian The system is installed with debian-11.6.0-amd64-netinst.iso\nneofetch\n _,met$$$$$gg. root ,g$$$$$$$$$$$$$$$P. --------- ,g$$P\" \"\"\"Y$$.\". OS: Debian GNU/Linux 11 (bullseye) x86_64 ,$$P' `$$$. Host: OptiPlex 3010 01 ',$$P ,ggs. `$$b: Kernel: 5.10.0-25-amd64 `d$$' ,$P\"' . $$$ Uptime: 6 mins $$P d$' , $$P Packages: 566 (dpkg) $$: $$. - ,d$$' Shell: bash 5.1.4 $$; Y$b._ _,d$P' Resolution: 1280x800 Y$$. `.`\"Y$$$$P\"' CPU: Intel i5-2300 (4) @ 2.800GHz `$$b \"-.__ GPU: Intel 2nd Generation Core Processor Family `Y$$ GPU: NVIDIA Tesla M40 `Y$$. Memory: 160MiB / 5828MiB `$$b. `Y$$b. `\"Y$b._ `\"\"\" swap A large swap partition is recommended. But it’s possible to add a secondary swap file alongside with swap partition to load RAM hungry models in any time. The advantage of swap file is that this is rather a temporary and flexible solution than a permanent fixed partition.\nfallocate -l 64G /home/swapfile chmod 600 /home/swapfile mkswap /home/swapfile swapon /home/swapfile nano /etc/fstab\nUUID=xxxxx-xxx swap swap defaults,pri=100 0 0 /home/swapfile swap swap defaults,pri=10 0 0 Check with swapon --show and free -h\ncuda Disable Nouveau driver\nbash -c \"echo blacklist nouveau  /etc/modprobe.d/blacklist-nvidia-nouveau.conf\" bash -c \"echo options nouveau modeset=0  /etc/modprobe.d/blacklist-nvidia-nouveau.conf\" update-initramfs -u update-grub reboot Install dependencies\napt install linux-headers-`uname -r` build-essential libglu1-mesa-dev libx11-dev libxi-dev libxmu-dev gcc software-properties-common sudo git python3 python3-venv pip libgl1 git-lfs -y (Optional) add sudoer usermod -aG sudo username then reboot\nInstall Nvidia\nwget https://developer.download.nvidia.com/compute/cuda/repos/debian11/x86_64/cuda-keyring_1.1-1_all.deb dpkg -i cuda-keyring_1.1-1_all.deb add-apt-repository contrib apt-get update apt-get -y install cuda (Optional) Fix if the keyring doesn’t work automatically\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/debian11/x86_64/7fa2af80.pub sudo bash -c 'echo \"deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos/debian11/x86_64/ /\"  /etc/apt/sources.list.d/cuda.list' apt-get update apt-get -y install cuda After cuda installed run sudo update-initramfs -u and nano ~/.bashrc\nexport PATH=/usr/local/cuda-12.2/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64\\ ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} ldconfig, source ~/.bashrc or reboot then\nnvidia-smi, nvcc --version and lspci to verify if everything is working\n00:00.0 Host bridge: Intel Corporation 2nd Generation Core Processor Family DRAM Controller (rev 09) 00:01.0 PCI bridge: Intel Corporation Xeon E3-1200/2nd Generation Core Processor Family PCI Express Root Port (rev 09) 00:02.0 VGA compatible controller: Intel Corporation 2nd Generation Core Processor Family Integrated Graphics Controller (rev 09) 00:1a.0 USB controller: Intel Corporation 6 Series/C200 Series Chipset Family USB Enhanced Host Controller #2 (rev 04) 00:1b.0 Audio device: Intel Corporation 6 Series/C200 Series Chipset Family High Definition Audio Controller (rev 04) 00:1c.0 PCI bridge: Intel Corporation 6 Series/C200 Series Chipset Family PCI Express Root Port 1 (rev b4) 00:1c.4 PCI bridge: Intel Corporation 6 Series/C200 Series Chipset Family PCI Express Root Port 5 (rev b4) 00:1d.0 USB controller: Intel Corporation 6 Series/C200 Series Chipset Family USB Enhanced Host Controller #1 (rev 04) 00:1f.0 ISA bridge: Intel Corporation H61 Express Chipset LPC Controller (rev 04) 00:1f.2 SATA controller: Intel Corporation 6 Series/C200 Series Chipset Family 6 port Desktop SATA AHCI Controller (rev 04) 00:1f.3 SMBus: Intel Corporation 6 Series/C200 Series Chipset Family SMBus Controller (rev 04) 03:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller (rev 06) 01:00.0 3D controller: NVIDIA Corporation GM200GL [Tesla M40] (rev a1) Subsystem: NVIDIA Corporation GM200GL [Tesla M40] Flags: bus master, fast devsel, latency 0, IRQ 16 Memory at f1000000 (32-bit, non-prefetchable) [size=16M] Memory at 800000000 (64-bit, prefetchable) [size=32G] Memory at 400000000 (64-bit, prefetchable) [size=32M] Capabilities: [60] Power Management version 3 Capabilities: [68] MSI: Enable- Count=1/1 Maskable- 64bit+ Capabilities: [78] Express Endpoint, MSI 00 Capabilities: [100] Virtual Channel Capabilities: [258] L1 PM Substates Capabilities: [128] Power Budgeting  Capabilities: [420] Advanced Error Reporting Capabilities: [600] Vendor Specific Information: ID=0001 Rev=1 Len=024  Kernel driver in use: nvidia Kernel modules: nouveau, nvidia_drm, nvidia With the BIOS Mod, neither Above 4G Decoding nor pci=realloc is needed.\nAs memtioned, this mod is only for Linux. Under Windows, the GPU still gets Error code 12. It’s possible to virtualize Windows as a VM with GPU passthrough, like on Proxmox, but I didn’t try that.\nAsuka benchmark runs at 0:25 per asuka and 2.0it/s\nSubs AI Whisper My Wisper server was using Generate-subtitles and it had done a lot of work. However, that project is outdated and now I find Subs AI is better in every way.\nInstall everything needed\nsudo apt install ffmpeg pip install setuptools-rust pip install git+https://github.com/abdeladim-s/subsai subsai-webui Run subsai-webui --server.maxMessageSize 500 to increase the upload size limit, subsai-webui to start server\n(Optional) Add to PATH nano ~/.bashrc then source ~/.bashrc\n'/home/aier/.local/bin' export PATH=\"$HOME/.cargo/bin:$PATH\" export PATH=\"$HOME/.local/bin:$PATH\" (Optional) Fixing cudnn issue by sudo apt-get install libcudnn8\nCould not load library libcudnn_ops_infer.so.8. Error: libcudnn_ops_infer.so.8: cannot open shared object file: No such file or directory Please make sure libcudnn_ops_infer.so.8 is in your library path! Text generation web UI Since I would like to explore and test out many different LLMs, oobabooga’s Text generation web UI would be a great way to do that.\ngit clone https://github.com/oobabooga/text-generation-webui.git cd text-generation-webui ./start_linux.sh nano CMD_FLAGS.txt to make it online with flags --listen --listen-host 0.0.0.0 --listen-port 7860\nCheck Hugging Face’s Leaderboard\nOpenCompass’s Leaderboard\nFor beginers: Free Open-Source AI LLM Guide (Summer 2023)\nAPI Install requirements from ~/text-generation-webui/extensions/api\npip install -r requirements.txt Edit flags nano CMD_FLAGS.txt\n--listen --listen-host 0.0.0.0 --listen-port 7860 --api --extensions openai Optional for OpenedAI API nano extensions/openai/.env\nOPENAI_API_KEY=sk-111111111111111111111111111111111111111111111111 OPENAI_API_BASE=http://0.0.0.0:5001/v1 Then start the server as usual, and it would be able to talk to other compatible services.\nLlama2 uncensored Use Llama2 without Meta’s non-sense agreement and censorship.\nGo to http://ip:7860 → Switch to Model tab → Under Download model or LoRA → Paste TheBloke/llama2_7b_chat_uncensored-GGUF → llama2_7b_chat_uncensored.Q5_K_M.gguf → Click Get file list then Download\nFlags/Parameters for llama.cpp\n Checking CPU is a must to avoid Illegal instruction n-gpu-layers to maximum 128 n_ctx limits the prompt length, costs VRAM threads does not matter n_batch does not matter RoPE options are left default mul_mat_q speed up a bit no-mmap can doom the speed like crawling in hell mlock slow down speed a bit  Flags for Transformers (e.g. bloomz-1b7)\n compute_dtype to float32 is the only change needed  The speed of generation is around 10-15 tokens/s for 7B models and 3-9 tokens/s for 13B modles.\nDespite the 7B version of Llama2 (5000+ MB), and M40 can handle large sized 13B models very easily (e.g. wizardlm-1.0-uncensored-llama2-13b.Q5_K_M 11000-14000 MB).\nFor long context/tokens models, 7b-128k or 13b-64k models are feasible. Allocating n_ctx value wisely to prevent running out of memory.\nConvert Models Due to the compatibility, GPTQ-for-LLaMa and AutoGPTQ doesn’t work well for old cards and sometimes I can only find models with the old GGML model which is obsoleted. Instead of relying on TheBloke, I’d do it myself.\nGGML to GGUF\ngit clone https://github.com/ggerganov/llama.cpp cd ~/text-generation-webui/models/ wget https://huggingface.co/s3nh/llama2_13b_chat_uncensored-GGML/resolve/main/llama2_13b_chat_uncensored.ggmlv3.q5_0.bin python3 /home/username/llama.cpp/convert-llama-ggml-to-gguf.py -i llama2_13b_chat_uncensored.ggmlv3.q5_0.bin -o llama2_13b_chat_uncensored.ggmlv3.q5_0.gguf Llama-2-7B-32K-Instruct The long context model I choose is togethercomputer/Llama-2-7B-32K-Instruct. It loads by Transformerswith enable use_fast to function normally. TheBloke/Llama-2-7B-32K-Instruct-GGUF doesn’t work for me.\nThe performance and results is really good:\nOutput generated in 22.14 seconds (9.53 tokens/s, 211 tokens, context 294) Output generated in 324.32 seconds (8.53 tokens/s, 2765 tokens, context 66) mistral-7b-instruct-v0.1.Q5_K_M.gguf\nOutput generated in 30.74 seconds (12.95 tokens/s, 398 tokens, context 69) TTS Generation WebUI Using the recommend installer\nwget https://github.com/rsxdalv/one-click-installers-tts/archive/refs/tags/v6.0.zip sudo chmod +x v6.0.zip unzip v6.0.zip cd one-click-installers-tts-6.0 ./start_linux.sh Bark Voice Clone It can take a while and use the time to prepare a 15-30s voice sample.\nWhen it’s done: Go to http://ip:7860 → Switch to Bark Voice Clone tab → Upload Input Audio → Click Generate Voice → Click Use as history → Switch to Generation (Bark) tab → Click refresh button → Select the .npz sample inAudio Voice → Click Generate\nNow, it’s time to start experimenting temperatures. Save the sample when satisfied.\nNote: It will download required files while the first use so watch the output from backend between clicking.\nMusicGen Recent years, there is a emerge of AI generated music videos with obviously SD generated cover image on YouTube. I believe they are made with MusicGen.\nSo I made a few my own taste of music with prompt Chiptune, KEYGEN, 8bit and that sounds not bad.\nh2oGPT As a researcher, I work with a lot of ebooks and documents on a daily basis. A private offline version of pdfGPT or chatpdf is extremely helpful.\nUnfortunately, Text generation web UI is falling far behind for this specific task. By comparison, h2oGPT is by far the most advanced project.\nTo install:\ngit clone https://github.com/h2oai/h2ogpt.git cd h2ogpt pip install -r requirements.txt pip install -r reqs_optional/requirements_optional_langchain.txt pip install -r reqs_optional/requirements_optional_gpt4all.txt pip install pysqlite3-binary chromadb chroma-hnswlib hnswlib auto_gptq==0.4.2 python3 generate.py Solution to RuntimeError: Your system has an unsupported version of sqlite3. Chroma requires sqlite3 = 3.35.0.\nAdded these 3 lines at the beginning: nano nano /home/user/.local/lib/python3.9/site-packages/chromadb/__init__.py\n__import__('pysqlite3') import sys sys.modules['sqlite3'] = sys.modules.pop('pysqlite3') When it’s done: Go to http://ip:7860 → Switch to Models tab → Choose Base Model h2oai/h2ogpt-4096-llama2-7b-chat → Click Download/Load Model\nNext time, launching with python3 generate.py --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --score_model=None --langchain_mode='UserData' --user_path=user_path\n(Optional) Launch in offline mode by python3 generate.py --score_model=None --gradio_size=small --model_lock=\"[{'base_model': 'h2oai/h2ogpt-4096-llama2-7b-chat'}]\" --save_dir=save_fastup_chat --prepare_offline_level=2\nNote: Due to instructor-large embedding and increased context length, h2oai/h2ogpt-4096-llama2-13b-chat and other 13B models end up taking more than 26GB VRAM, which is out of M40’s range. So 7B models like h2ogpt-4096-llama2-7b-chat, ehartford/WizardLM-7B-Uncensored or finetuned h2ogpt-oasst1-4096-llama2-7b are more appropriate (costs 14GB VRAM).\nCustom Models h2oGPT web UI provides a large selections of models from huggingface. However, I would like to have some good non-English models in addition.\nNote:\n Unlike FAQ indicated, pre-downloading models to local and passing --model_path is unnecessary. Fine-tuned models is more likely to avoid trashy output For text translation/interpretation/summary, instruction finetuned models are better than chat finetuned models Use --lora_weights= to load a LoRA, --use_safetensors=True when load safetensors Long context version is preferred Recommend to add --prompt_type for custom models following FAQ and this family chart or this evolutionary graph If no prompt type preset, e.g. bloomz-7b1-mt, load the model without passing --prompt_type, go to Expert tab and experiment prompt type in the web ui It shares the same directory as other projects for HF model storage at ~/.cache/huggingface/hub/  Non-English Models Since vanilla Llama2 does not work well responding non-English languages, I have tested a list of Chinese/multilingual LLMs and found some useful result. Others may do the same for their preferred language:\n LinkSoul/Chinese-Llama-2-7b+zigge106/LinkSoul-Chinese-Llama-2-7b-200wechat-chatgpt-best: Chats ok but extremely bad performance working with context, with or without LoRA, unable to complete any test THUDM/chatglm2-6bor Echolist-yixuan/chatglm2-6b-qlora: Chats ok but only generates garbage for context, unable to complete any test ziqingyang/chinese-alpaca-2-7b+ziqingyang/chinese-alpaca-2-lora-7b: good performance, no good prompt type with or without LoRA, language disturbance, only hallucinates if not generates garbage, typical Artificial Imbecility baichuan-inc/Baichuan2-7B-Chat: prompt type mptinstruct, openai_chat, wizard2 and etc., good performance, good intelligence but hallucinates, normal accuracy Linly-AI/Chinese-LLaMA-2-7B-hf: prompt type llama2 doesn’t work, instead use instruct, quality, gptj and etc., good performance and okay intelligence, bad accuracy OpenBuddy/openbuddy-openllama-7b-v12-bf16: prompt type openai_chat, quality, gptj and etc., good performance, normal intelligence, normal accuracy, recommend FreedomIntelligence/phoenix-inst-chat-7b: prompt type guanaco, open_assistant, wizard_lm and etc., good performance, normal intelligence, normal accuracy, recommend BelleGroup/BELLE-7B-2M: prompt type guanaco, instruct, beluga and etc., normal performance, bad intelligence, bad accuracy Qwen/Qwen-7B-Chat: prompt type wizard_lm, quality, wizard3 and etc., good performance, great intelligence but overly creative, censored, bad accuracy, worth a try xverse/XVERSE-7B-Chat: prompt type one_shot, mptinstruct, gptj and etc., normal performance, good intelligence but overly creative, bad accuracy, recommend internlm/internlm-chat-7b:low performance, no good prompt type, strong language disturbance, resists against language preset, only hallucinates if not generates garbage, typical Artificial Imbecility PengQu/Llama-2-7b-vicuna-Chinese: Prompt type instruct_vicuna, open_assistant, instruct_vicuna2 and etc., low performance, good intelligence but creative, hallucinates, normal accuracy, does not avoid violence  The average accuracy is not satisfying probably due to 7B size. Tinkering configurations in Expert tab may help. Further custom training may be required as well.\nModify System/Query/Summary (Pre-)Prompt under Expert tab and Prompt (or Custom) in web ui or passing --prompt_dict accordingly to fit non-English language preference:\n System Prompt: 用中文回答问题或响应指令 Query Pre-Prompt: 请注意并记住下面的信息，这将有助于在情境结束后回答问题或遵循命令。 Query Prompt: 根据上文提供的文档来源中的信息， Summary Pre-Prompt: 为了撰写简明扼要的单段或项目符号列表摘要，请注意以下文本 Summary Prompt: 仅利用上述文档来源中的信息，写一个简明扼要的关键结果总结（最好使用项目符号）：  ",
  "wordCount" : "3998",
  "inLanguage": "en",
  "datePublished": "2023-10-23T00:00:00Z",
  "dateModified": "2023-10-23T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Jun"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://techshinobi.org/posts/cheapai/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Tech Shinobi",
    "logo": {
      "@type": "ImageObject",
      "url": "https://techshinobi.org/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://techshinobi.org/" accesskey="h" title="Tech Shinobi (Alt + H)">Tech Shinobi</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://techshinobi.org/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://techshinobi.org/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://techshinobi.org/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://techshinobi.org/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://techshinobi.org/">Home</a>&nbsp;»&nbsp;<a href="https://techshinobi.org/posts/">Posts</a></div>
    <h1 class="post-title">
      Cheapskate&#39;s Homebrew AI Lab
    </h1>
    <div class="post-meta"><span title='2023-10-23 00:00:00 +0000 UTC'>October 23, 2023</span>&nbsp;·&nbsp;19 min&nbsp;·&nbsp;Jun

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#old-stories" aria-label="Old Stories">Old Stories</a><ul>
                        
                <li>
                    <a href="#philosophy" aria-label="Philosophy">Philosophy</a></li></ul>
                </li>
                <li>
                    <a href="#hardware" aria-label="Hardware">Hardware</a><ul>
                        
                <li>
                    <a href="#gpu-selection" aria-label="GPU Selection">GPU Selection</a></li>
                <li>
                    <a href="#about-power-supply" aria-label="About Power Supply">About Power Supply</a></li>
                <li>
                    <a href="#thermal-mod-zip-tie-method" aria-label="Thermal MOD (Zip-tie Method)">Thermal MOD (Zip-tie Method)</a></li></ul>
                </li>
                <li>
                    <a href="#firmware" aria-label="Firmware">Firmware</a><ul>
                        
                <li>
                    <a href="#files" aria-label="Files">Files</a></li></ul>
                </li>
                <li>
                    <a href="#software" aria-label="Software">Software</a><ul>
                        
                <li>
                    <a href="#debian" aria-label="Debian">Debian</a><ul>
                        
                <li>
                    <a href="#swap" aria-label="swap">swap</a></li>
                <li>
                    <a href="#cuda" aria-label="cuda">cuda</a></li></ul>
                </li>
                <li>
                    <a href="#subs-ai-whisper" aria-label="Subs AI Whisper">Subs AI Whisper</a></li>
                <li>
                    <a href="#text-generation-web-ui" aria-label="Text generation web UI">Text generation web UI</a><ul>
                        
                <li>
                    <a href="#api" aria-label="API">API</a></li>
                <li>
                    <a href="#llama2-uncensored" aria-label="Llama2 uncensored">Llama2 uncensored</a><ul>
                        
                <li>
                    <a href="#convert-models" aria-label="Convert Models">Convert Models</a></li></ul>
                </li>
                <li>
                    <a href="#llama-2-7b-32k-instruct" aria-label="Llama-2-7B-32K-Instruct">Llama-2-7B-32K-Instruct</a></li></ul>
                </li>
                <li>
                    <a href="#tts-generation-webui" aria-label="TTS Generation WebUI">TTS Generation WebUI</a><ul>
                        
                <li>
                    <a href="#bark-voice-clone" aria-label="Bark Voice Clone">Bark Voice Clone</a></li>
                <li>
                    <a href="#musicgen" aria-label="MusicGen">MusicGen</a></li></ul>
                </li>
                <li>
                    <a href="#h2ogpt" aria-label="h2oGPT">h2oGPT</a><ul>
                        
                <li>
                    <a href="#custom-models" aria-label="Custom Models">Custom Models</a></li>
                <li>
                    <a href="#non-english-models" aria-label="Non-English Models">Non-English Models</a>
                </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="old-stories">Old Stories<a hidden class="anchor" aria-hidden="true" href="#old-stories">#</a></h2>
<p>The computer hardware used to be more playful and worth tinkering. My favorate platfom from a decade ago, Sandy Bridge on LGA 1155, can still be powerful even today.</p>
<p>Back in the days, I used to repair people&rsquo;s electronics for free. Because of that, I also received a lots of spares and e-waste in exchange. One of the best was a LGA 1155 motherboard with i5-2300 on it. Then, I bought a cheap E3-1245 and GTX 750 Ti to make it a gaming rig. I played a lot of games on that, such as Dark Souls series and Metro series. Before it was sold, last games I played on this build was Metro Exodus and Elden Ring.</p>
<p>Sandy Bridge was the last generation using soldered integrated heat spreader (IHS) in a long time. It&rsquo;s cooler and stays cool over time way better than its thermal-pasted successors (e.g. Haswell/Hotwell). Back then, there was a doggerel joking about AMD&rsquo;s performance and Intel&rsquo;s temperature, saying &ldquo;Unlocking AMD, Delidding Intel.&rdquo;</p>
<p>The real fun project was a LGA771 to LGA775 Mod on a Dell Vostro 220s. It was a hardware hack that letting this low-end Vostro uses a dirt cheap yet powerful Xeon. The motherboard also has 4 SATA ports so it works well as a NAS.</p>
<p>Next is our today&rsquo;s topic, Dell Optiplex 3010 SFF. It was on sell for quite a few months but seemingly no one wants it at all. I don&rsquo;t think it&rsquo;s a completely garbage comparing the Vostro above. It came with an i3-3220 but has been upgraded to the i5-2300 that was replaced from the gaming rig for the sake of LGA 1155. I&rsquo;m not planning to put an Xeon E3 on it but a weird graphic card without output port.</p>
<h3 id="philosophy">Philosophy<a hidden class="anchor" aria-hidden="true" href="#philosophy">#</a></h3>
<p>If you&rsquo;re wondering—why bother spending such amount of time and effort tinkering those e-wastes? Just because of being a cheapskate and saving money?</p>
<p>Here is the answer from <a href="https://techshinobi.org/posts/pmos/#0x00-before-start">my previous post</a>:</p>
<blockquote>
<p>I really like the idea from <a href="https://cheapskatesguide.org/articles/know-what-youre-buying.html">cheapskatesguide</a> and <a href="https://solar.lowtechmagazine.com/2020/12/how-and-why-i-stopped-buying-new-laptops.html">lowtechmagazine</a> that could save people from the pitfalls of consumerism. Moreover, to me it&rsquo;s not about saving money on technology nor adapting certain lifestyles. This is a manifestation of ideology, the way of pursuing freedom.</p>
</blockquote>
<p>By this chance, I would like to add more details on that.</p>
<blockquote>
<p>On the internet, proprietary software isn’t the only way to lose your freedom. Service as a Software Substitute, or SaaSS, is another way to let someone else have power over your computing.</p>
<p>If you use SaaSS, the server operator controls your computing. It requires entrusting all the pertinent data to the server operator, which will be forced to show it to the state as well—who does that server really serve, after all?</p>
<p>SaaSS does not require covert code to obtain the user’s data. Instead, users must send their data to the server in order to use it. This has the same effect as spyware: the server operator gets the data—with no special effort, by the nature of SaaSS.</p>
<p>With SaaSS, the server operator can change the software in use on the server. He ought to be able to do this, since it’s his computer; but the result is the same as using a proprietary application program with a universal back door: someone
has the power to silently impose changes in how the user’s computing gets done.</p>
<p>Thus, SaaSS is equivalent to running proprietary software with spyware and a universal back door. It gives the server operator unjust power over the user, and that power is something we must resist.</p>
<p>SaaSS always subjects you to the power of the server operator, and the only remedy is, Don’t use SaaSS! Don’t use someone else’s server to do your own computing on data provided by you.</p>
</blockquote>
<p>These are selected from <a href="https://www.gnu.org/doc/fsfs3-hardcover.pdf">Free Software, Free Society</a> and I recommend to read the whole book if you have not already.</p>
<p>RMS was right, once again. SaaS is dangerous to human liberty and I was aware of it. Since it was called &ldquo;the cloud&rdquo;, I&rsquo;ve been self-hosting and peer-to-peer everything back that time.</p>
<p>Nowadays, it&rsquo;s called &ldquo;Generative AI&rdquo;. This is why the first chapter of <a href="https://techshinobi.org/posts/cheapsd/#no-dall-e-no-midjourney-and-no-colab">my Stable Diffusion article</a> called &ldquo;No DALL-E, No Midjourney and No Colab&rdquo; where talks about neutrality and transparency. AIaaS <a href="https://www.code42.com/blog/is-your-data-leaking-via-chatgpt/">leaks data</a> and <a href="https://arxiv.org/abs/2304.14475">not secure</a>. Not only ChatGPT can become <a href="https://arxiv.org/abs/2304.12298">BadGPT</a>, but also open-source LLM can become <a href="https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/">PoisonGPT</a>. Just like Diffusion models <a href="https://huggingface.co/docs/hub/security-malware">can have malware</a>. Never blindly trust something just because it&rsquo;s open-source.</p>
<h2 id="hardware">Hardware<a hidden class="anchor" aria-hidden="true" href="#hardware">#</a></h2>
<p><em>Note: This is rather a rough record than a proper guide. Keep in mind, be sure you have enough experience tinkering PC hardware. Proceed with caution and at your own risk.</em></p>
<p>During the summer, I was working on my research paper and now I have some time back to this fun project.</p>
<p>Although I had some fun and had done quite some projects with it, the biggest con about <a href="https://techshinobi.org/posts/cheapsd/#my-mini-server-build">My mini Server</a> is the VRAM capacity. 4GB is too limiting when I attempt to do training, like LoRA models. Not only that, even using <a href="https://techshinobi.org/posts/cheapsd/#controlnet">ControlNet</a> or running <a href="https://github.com/Hannibal046/Awesome-LLM#open-llm">Open LLM</a> are too restrained. So I decide to buy a NVIDIA Tesla M40, with 24 GB of VRAM which is the largest amount I can get from a cheap single card.</p>
<p>Money on the parts ($180-250 in total) :</p>
<ul>
<li>Dell Optiplex (Sandy Bridge), $0
<ul>
<li>In my case it&rsquo;s 3010 SFF but other models (7010/9010) would be better</li>
<li>On Ebay $40 for whole unit, $20 for motherboard only</li>
<li>These models apears in surplus or thrift store quite often</li>
<li>With the BIOS hack, neither <code>Above 4G Decoding</code>, <code>Resizable BAR</code> nor <code>pci=realloc</code> are needed</li>
</ul>
</li>
<li>NVIDIA Tesla M40 24 GB, $110</li>
<li>Corsair AIO Cooler, $25
<ul>
<li>It&rsquo;s the cheapest I can find, unknown model</li>
<li>The condition is working but looks pretty much AS-IS</li>
<li>I refilled it with purified water, checked the seal and pressure tested the pump</li>
<li>No mounting bracket and I don&rsquo;t need it either</li>
<li>The screw holes on M40 is 58 x  58 mm, the smaller cooler surface the better (need space to put small heatsinks on VRAM chips)</li>
<li>A $17 NZXT Kraken G12 is the proper way to go, but those compatible coolers can be expensive even buying used</li>
<li>Most CPU AIO coolers would do it though, if using zip tie method</li>
<li>Regular CPU air coolers may be too heavy for our card, and the blower fan adapter method is loud, ineffecient yet not cheap</li>
</ul>
</li>
<li>Seasonic SSR-550RM, $28
<ul>
<li>Just a little bit overkill but it&rsquo;s a good deal</li>
</ul>
</li>
<li>CPU 8 Pin EPS Cable for Seasonic Modular PSU, $10</li>
<li>Dual PSU Adapter, $9
<ul>
<li>Optional, bought it for convenience</li>
</ul>
</li>
<li>1TB SATA SSD, $0
<ul>
<li>Optional, possible to hack in NVMe drives for larger form like 7010/9010</li>
</ul>
</li>
</ul>
<h3 id="gpu-selection">GPU Selection<a hidden class="anchor" aria-hidden="true" href="#gpu-selection">#</a></h3>
<p>Depending on price, availability and capacity, only K80, M40 and P40 are in my options.</p>
<pre tabindex="0"><code>7xx(Kepler)：Tesla K80 24G
9xx(Maxwell)：Tesla M40 12G/24G，Tesla M4 4G
10x0(Pascal)：Tesla P100 16G，Tesla P40 24G，Tesla P4 8G
20x0(Volta/Turing)：Tesla T4 16G，Tesla V100 16G/32G
30x0(Ampere)：NVIDIA A100 40/80GB，NVIDIA A40 48GB，NVIDIA RTX A6000 48G
</code></pre><p>The best bang for the budget seems like the old good K80, but it isn&rsquo;t. The driver for Kepler cards are stucked with 440.95.01 and CUDA Toolkit 11.4. Besides that, K80 24G version is actually two 12G versions so it may only recognize half of the VRAM in some application.</p>
<p>The problem with Pascal card is that costs more money but not worth it. The support of <a href="https://moocaholic.medium.com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407">quantization matters</a> when it comes to AI inference. However, in order to get the performance gain from utilizing quantization. We are not only need to have the supported hardware and use the correct model, but the software needs to support it as well. Eventhough P100 supports FP16 and P40 supports INT8, they are <a href="https://rentry.co/stablediffgpubuy#hentai-for-deaf-cheapskates">not well supported</a> by the <a href="https://discuss.pytorch.org/t/cnn-fp16-slower-than-fp32-on-tesla-p100/12146">upstream</a>.</p>
<p>Therefore, if using INT8 or FP16 is not expected, because most of the time it runs with FP32, no reason to spend more on P100 and P40. M40 is the sweet spot without doubt, and it saves a lot of hassle on optimazation. All in all, M40 is great for non-production evironment where performance isn&rsquo;t the priority.</p>
<h3 id="about-power-supply">About Power Supply<a hidden class="anchor" aria-hidden="true" href="#about-power-supply">#</a></h3>
<p><img loading="lazy" src="images/On_Power_Connections.png" alt="On Power Connections"  />

Because there is only a 4+4 pin CPU power cable on my 550w PSU and Tesla cards require CPU&rsquo;s EPS connection. I bought a $7 spliting PCIe to EPS adapter. Sadly, it melt within minutes.</p>
<p><img loading="lazy" src="images/melt_cable.JPG" alt="melt cable"  />
</p>
<p>It&rsquo;s very likely that the melting was caused by the copper in the cable was too thin or the resistance of the cable was too high. To power the 250w GPU, a thicker stronger cable is required.</p>
<p>I think it&rsquo;s possible to force the 4+4 pin male connector fit into the 8 pin female socket on the GPU by filing it, but didn&rsquo;t try.
Finally, I bought a dedicated modular cable instead of another adapter. It says &ldquo;UL1007 18AWG tinned copper wire with high current terminal&rdquo; that sounds strong enough for the task and it indeed does.</p>
<p>The dual PSU adapter is more convenient than a switch jumper cable and safer than a paper clip.</p>
<h3 id="thermal-mod-zip-tie-method">Thermal MOD (Zip-tie Method)<a hidden class="anchor" aria-hidden="true" href="#thermal-mod-zip-tie-method">#</a></h3>
<p><img loading="lazy" src="images/zip_tie_method.JPG" alt="zip tie method"  />
</p>
<p><img loading="lazy" src="images/zip_tie_method2.JPG" alt="zip tie method2"  />
</p>
<p>When hardware is incompatible, zip tie is our best friend.</p>
<p>I have talked a lot on the cooling system already. Here just a showcase for my zip tie method mounting any sized AIO cooler, case fan and heat sink.</p>
<p>It is quite stable and low-noise. Works on both CPU and GPU. As a bounus, the zip ties also hold up the GPU backplate quite well, and the water pipes can support the GPU to balance its weight if tweaked to a good angle. So, nothing will come loose or bend in a long run.</p>
<p><img loading="lazy" src="images/system_overlook.JPG" alt="system overlook"  />
</p>
<p>In my CASE, the cardboard box, the cooling is a bit overkill and I&rsquo;m quite happy about 23-26C idle and 46-52C full-load temperature.</p>
<p>Very cool temperature on idle:</p>
<pre tabindex="0"><code>+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla M40 24GB                 On  | 00000000:01:00.0 Off |                    0 |
| N/A   24C    P8              18W / 250W |      0MiB / 23040MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
</code></pre><p>Full-load running Llama2:</p>
<pre tabindex="0"><code>+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla M40 24GB                 On  | 00000000:01:00.0 Off |                    0 |
| N/A   52C    P0             250W / 250W |  16777MiB / 23040MiB |    100%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A    351272      C   python3                                   16774MiB |
+---------------------------------------------------------------------------------------+
</code></pre><h2 id="firmware">Firmware<a hidden class="anchor" aria-hidden="true" href="#firmware">#</a></h2>
<p><a href="https://github.com/xCuri0/ReBarUEFI/issues/11">The report of 7010</a> is very brief, so I decide to extend it a little bit based on my 3010 reimplementation.</p>
<p>First off follow <a href="https://www.tachytelic.net/2021/12/dell-optiplex-7010-pcie-nvme/">this NVME guide</a> to dump the BIOS rom.</p>
<p>On my 3010, the jumper needs to remove CPU cooler(take the entire cooler with the fan) to find.</p>
<p><img loading="lazy" src="service_mode_jumper.JPG" alt="service mode jumper"  />
</p>
<p>According to Dell official <code>2-3 pin: normal default; 1-2 pin: clear ME(service mode)</code> to move the jumper. It is actually  MECLR1 on my board (right side of the photo).</p>
<p>After updated my BIOS version from A09 to A20, I made a backup using <code>fptw64.exe -d backup.bin</code>.</p>
<p>Next, follow <a href="https://github.com/xCuri0/ReBarUEFI/wiki/DSDT-Patching#dsdt-modification">this DSDT guide</a> but during step 5, after finishing the changes, add some more extra changes from <a href="https://github.com/xCuri0/ReBarUEFI/issues/77">this issue</a>:</p>
<ul>
<li>Change <code>If (((MM64 == Zero) || (OSYS &lt;= 0x07D3)))</code> to <code>If (((OSYS &lt;= 0x07D3)))</code></li>
<li>Change <code>ElseIf (E4GM)</code> to <code>Else</code></li>
<li>Remove this section</li>
</ul>
<pre tabindex="0"><code>                Else
                {
                    CreateDWordField (BUF0, \_SB.PCI0._Y0F._LEN, M4LN)  // _LEN: Length
                    M4LN = Zero
                }
</code></pre><p>Continue rest of the steps to finish the DSDT mod, then proceed with <a href="https://github.com/xCuri0/ReBarUEFI/wiki/Using-UEFIPatch#using-uefipatch">UEFI Patch guide</a> to get the patched version of the mod rom.</p>
<p>Finally, use <code>fptw64.exe -bios -f modpatched.bin</code> to flash the rom.</p>
<h3 id="files">Files<a hidden class="anchor" aria-hidden="true" href="#files">#</a></h3>
<p>I have share all files generated during the process in <a href="https://github.com/Jun-TheTechShinobi/Dell-Optiplex-3010-ReBarUEFI">this repo</a>.</p>
<p><a href="https://github.com/Jun-TheTechShinobi/Dell-Optiplex-3010-ReBarUEFI/blob/main/A20MOD.rom.patched.bin">A20MOD.rom.patched.bin</a> is the binary file named <code>modpatched.bin</code> in the final step above. The souce code is in <a href="https://github.com/Jun-TheTechShinobi/Dell-Optiplex-3010-ReBarUEFI/blob/main/DSDTMod.dsl">DSDTMod.dsl</a>.</p>
<h2 id="software">Software<a hidden class="anchor" aria-hidden="true" href="#software">#</a></h2>
<h3 id="debian">Debian<a hidden class="anchor" aria-hidden="true" href="#debian">#</a></h3>
<p>The system is installed with <code>debian-11.6.0-amd64-netinst.iso</code></p>
<p>neofetch</p>
<pre tabindex="0"><code>       _,met$$$$$gg.          root 
    ,g$$$$$$$$$$$$$$$P.       --------- 
  ,g$$P&#34;     &#34;&#34;&#34;Y$$.&#34;.        OS: Debian GNU/Linux 11 (bullseye) x86_64 
 ,$$P&#39;              `$$$.     Host: OptiPlex 3010 01 
&#39;,$$P       ,ggs.     `$$b:   Kernel: 5.10.0-25-amd64 
`d$$&#39;     ,$P&#34;&#39;   .    $$$    Uptime: 6 mins 
 $$P      d$&#39;     ,    $$P    Packages: 566 (dpkg) 
 $$:      $$.   -    ,d$$&#39;    Shell: bash 5.1.4 
 $$;      Y$b._   _,d$P&#39;      Resolution: 1280x800 
 Y$$.    `.`&#34;Y$$$$P&#34;&#39;         CPU: Intel i5-2300 (4) @ 2.800GHz 
 `$$b      &#34;-.__              GPU: Intel 2nd Generation Core Processor Family 
  `Y$$                        GPU: NVIDIA Tesla M40 
   `Y$$.                      Memory: 160MiB / 5828MiB 
     `$$b.
       `Y$$b.                                         
          `&#34;Y$b._                                     
              `&#34;&#34;&#34;
</code></pre><h4 id="swap">swap<a hidden class="anchor" aria-hidden="true" href="#swap">#</a></h4>
<p>A large swap partition is recommended. But it&rsquo;s possible to add a secondary swap file alongside with swap partition to load RAM hungry models in any time. The advantage of swap file is that this is rather a temporary and flexible solution than a permanent fixed partition.</p>
<pre tabindex="0"><code>fallocate -l 64G /home/swapfile
chmod 600 /home/swapfile
mkswap /home/swapfile
swapon /home/swapfile
</code></pre><p>nano /etc/fstab</p>
<pre tabindex="0"><code>UUID=xxxxx-xxx swap swap defaults,pri=100 0 0
/home/swapfile swap swap defaults,pri=10 0 0
</code></pre><p>Check with <code>swapon --show</code> and <code>free -h</code></p>
<h4 id="cuda">cuda<a hidden class="anchor" aria-hidden="true" href="#cuda">#</a></h4>
<p>Disable Nouveau driver</p>
<pre tabindex="0"><code>bash -c &#34;echo blacklist nouveau &gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf&#34;
bash -c &#34;echo options nouveau modeset=0 &gt;&gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf&#34;
update-initramfs -u
update-grub
reboot
</code></pre><p>Install dependencies</p>
<pre tabindex="0"><code>apt install linux-headers-`uname -r` build-essential libglu1-mesa-dev libx11-dev libxi-dev libxmu-dev gcc software-properties-common sudo git python3 python3-venv pip libgl1 git-lfs -y
</code></pre><p>(Optional)  add sudoer <code>usermod -aG sudo username</code> then <code>reboot</code></p>
<p>Install Nvidia</p>
<pre tabindex="0"><code>wget https://developer.download.nvidia.com/compute/cuda/repos/debian11/x86_64/cuda-keyring_1.1-1_all.deb
dpkg -i cuda-keyring_1.1-1_all.deb
add-apt-repository contrib
apt-get update
apt-get -y install cuda
</code></pre><p>(Optional)  Fix if the keyring doesn&rsquo;t work automatically</p>
<pre tabindex="0"><code>sudo apt-key adv --fetch-keys  https://developer.download.nvidia.com/compute/cuda/repos/debian11/x86_64/7fa2af80.pub
sudo bash -c &#39;echo &#34;deb [signed-by=/usr/share/keyrings/cuda-archive-keyring.gpg] https://developer.download.nvidia.com/compute/cuda/repos/debian11/x86_64/ /&#34; &gt; /etc/apt/sources.list.d/cuda.list&#39;
apt-get update
apt-get -y install cuda
</code></pre><p>After cuda installed run <code>sudo update-initramfs -u</code> and <code>nano ~/.bashrc</code></p>
<pre tabindex="0"><code>export PATH=/usr/local/cuda-12.2/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64\
                         ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
</code></pre><p><code>ldconfig</code>, <code>source ~/.bashrc</code> or <code>reboot</code> then</p>
<p><code>nvidia-smi</code>, <code>nvcc --version</code> and <code>lspci</code> to verify if everything is working</p>
<pre tabindex="0"><code>00:00.0 Host bridge: Intel Corporation 2nd Generation Core Processor Family DRAM Controller (rev 09)
00:01.0 PCI bridge: Intel Corporation Xeon E3-1200/2nd Generation Core Processor Family PCI Express Root Port (rev 09)
00:02.0 VGA compatible controller: Intel Corporation 2nd Generation Core Processor Family Integrated Graphics Controller (rev 09)
00:1a.0 USB controller: Intel Corporation 6 Series/C200 Series Chipset Family USB Enhanced Host Controller #2 (rev 04)
00:1b.0 Audio device: Intel Corporation 6 Series/C200 Series Chipset Family High Definition Audio Controller (rev 04)
00:1c.0 PCI bridge: Intel Corporation 6 Series/C200 Series Chipset Family PCI Express Root Port 1 (rev b4)
00:1c.4 PCI bridge: Intel Corporation 6 Series/C200 Series Chipset Family PCI Express Root Port 5 (rev b4)
00:1d.0 USB controller: Intel Corporation 6 Series/C200 Series Chipset Family USB Enhanced Host Controller #1 (rev 04)
00:1f.0 ISA bridge: Intel Corporation H61 Express Chipset LPC Controller (rev 04)
00:1f.2 SATA controller: Intel Corporation 6 Series/C200 Series Chipset Family 6 port Desktop SATA AHCI Controller (rev 04)
00:1f.3 SMBus: Intel Corporation 6 Series/C200 Series Chipset Family SMBus Controller (rev 04)
03:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller (rev 06)
01:00.0 3D controller: NVIDIA Corporation GM200GL [Tesla M40] (rev a1)
        Subsystem: NVIDIA Corporation GM200GL [Tesla M40]
        Flags: bus master, fast devsel, latency 0, IRQ 16
        Memory at f1000000 (32-bit, non-prefetchable) [size=16M]
        Memory at 800000000 (64-bit, prefetchable) [size=32G]
        Memory at 400000000 (64-bit, prefetchable) [size=32M]
        Capabilities: [60] Power Management version 3
        Capabilities: [68] MSI: Enable- Count=1/1 Maskable- 64bit+
        Capabilities: [78] Express Endpoint, MSI 00
        Capabilities: [100] Virtual Channel
        Capabilities: [258] L1 PM Substates
        Capabilities: [128] Power Budgeting &lt;?&gt;
        Capabilities: [420] Advanced Error Reporting
        Capabilities: [600] Vendor Specific Information: ID=0001 Rev=1 Len=024 &lt;?&gt;
        Kernel driver in use: nvidia
        Kernel modules: nouveau, nvidia_drm, nvidia
</code></pre><p>With the BIOS Mod, neither <code>Above 4G Decoding</code> nor <code>pci=realloc</code> is needed.</p>
<p>As memtioned, this mod is only for Linux. Under Windows, the GPU still gets Error code 12. It&rsquo;s possible to virtualize Windows as a VM with GPU passthrough, like on Proxmox, but I didn&rsquo;t try that.</p>
<p><a href="https://techshinobi.org/posts/cheapsd/#asuka-benchmark">Asuka benchmark</a> runs at 0:25 per asuka and 2.0it/s</p>
<h3 id="subs-ai-whisper">Subs AI Whisper<a hidden class="anchor" aria-hidden="true" href="#subs-ai-whisper">#</a></h3>
<p>My Wisper server was <a href="https://techshinobi.org/posts/gensub-whisper/">using Generate-subtitles</a> and it had done a lot of work. However, that project is outdated and now I find <a href="https://github.com/abdeladim-s/subsai/">Subs AI</a> is better in every way.</p>
<p>Install everything needed</p>
<pre tabindex="0"><code>sudo apt install ffmpeg
pip install setuptools-rust
pip install git+https://github.com/abdeladim-s/subsai

subsai-webui
</code></pre><p>Run <code>subsai-webui --server.maxMessageSize 500</code> to increase the upload size limit, <code>subsai-webui</code> to start server</p>
<p>(Optional) Add to PATH <code>nano ~/.bashrc</code> then <code>source ~/.bashrc</code></p>
<pre tabindex="0"><code>&#39;/home/aier/.local/bin&#39;
export PATH=&#34;$HOME/.cargo/bin:$PATH&#34;
export PATH=&#34;$HOME/.local/bin:$PATH&#34;
</code></pre><p>(Optional) Fixing cudnn issue by <code>sudo apt-get install libcudnn8</code></p>
<pre tabindex="0"><code>Could not load library libcudnn_ops_infer.so.8. Error: libcudnn_ops_infer.so.8: cannot open shared object file: No such file or directory
Please make sure libcudnn_ops_infer.so.8 is in your library path!
</code></pre><h3 id="text-generation-web-ui">Text generation web UI<a hidden class="anchor" aria-hidden="true" href="#text-generation-web-ui">#</a></h3>
<p>Since I would like to explore and test out many different LLMs, <a href="https://github.com/oobabooga/text-generation-webui#text-generation-web-ui">oobabooga&rsquo;s Text generation web UI</a> would be a great way to do that.</p>
<pre tabindex="0"><code>git clone https://github.com/oobabooga/text-generation-webui.git
cd text-generation-webui
./start_linux.sh
</code></pre><p><code>nano CMD_FLAGS.txt</code> to make it online with flags <code>--listen --listen-host 0.0.0.0 --listen-port 7860</code></p>
<p>Check Hugging Face&rsquo;s <a href="https://huggingface.co/collections/open-llm-leaderboard">Leaderboard</a></p>
<p>OpenCompass&rsquo;s <a href="https://opencompass.org.cn/leaderboard-llm">Leaderboard</a></p>
<p>For beginers: <a href="https://lemmy.world/post/2219010">Free Open-Source AI LLM Guide (Summer 2023)</a></p>
<h4 id="api">API<a hidden class="anchor" aria-hidden="true" href="#api">#</a></h4>
<p>Install requirements from <code>~/text-generation-webui/extensions/api</code></p>
<pre tabindex="0"><code>pip install -r requirements.txt
</code></pre><p>Edit flags <code>nano CMD_FLAGS.txt</code></p>
<pre tabindex="0"><code>--listen --listen-host 0.0.0.0 --listen-port 7860 --api --extensions openai
</code></pre><p>Optional for <a href="https://github.com/oobabooga/text-generation-webui/tree/2d97897a2567df339caaaf74c5f3e2363fcf3e72/extensions/openai">OpenedAI API</a> <code>nano extensions/openai/.env</code></p>
<pre tabindex="0"><code>OPENAI_API_KEY=sk-111111111111111111111111111111111111111111111111
OPENAI_API_BASE=http://0.0.0.0:5001/v1
</code></pre><p>Then start the server as usual, and it would be able to talk to other compatible services.</p>
<h4 id="llama2-uncensored">Llama2 uncensored<a hidden class="anchor" aria-hidden="true" href="#llama2-uncensored">#</a></h4>
<p>Use Llama2 without Meta&rsquo;s non-sense agreement and censorship.</p>
<p>Go to <code>http://ip:7860</code> → Switch to <code>Model</code> tab → Under <code>Download model or LoRA</code> → Paste <code>TheBloke/llama2_7b_chat_uncensored-GGUF</code> → <code>llama2_7b_chat_uncensored.Q5_K_M.gguf</code> → Click <code>Get file list</code> then <code>Download</code></p>
<p>Flags/Parameters for <code>llama.cpp</code></p>
<ul>
<li>Checking <code>CPU</code> is a must to avoid <code>Illegal instruction</code></li>
<li><code>n-gpu-layers</code>  to maximum <code>128</code></li>
<li><code>n_ctx</code> limits the prompt length, costs VRAM</li>
<li><code>threads</code> does not matter</li>
<li><code>n_batch</code> does not matter</li>
<li><code>RoPE</code> options are left default</li>
<li><code>mul_mat_q</code> speed up a bit</li>
<li><code>no-mmap</code> can doom the speed like crawling in hell</li>
<li><code>mlock</code> slow down speed a bit</li>
</ul>
<p>Flags for <code>Transformers</code> (e.g. bloomz-1b7)</p>
<ul>
<li><code>compute_dtype</code> to float32 is the only change needed</li>
</ul>
<p>The speed of generation is around 10-15 tokens/s for 7B models and 3-9 tokens/s for 13B modles.</p>
<p>Despite the 7B version of Llama2 (5000+ MB), and M40 can handle large sized 13B models very easily (e.g. <code>wizardlm-1.0-uncensored-llama2-13b.Q5_K_M</code> 11000-14000 MB).</p>
<p>For long context/tokens models, 7b-128k or 13b-64k models are feasible. Allocating <code>n_ctx</code> value wisely to prevent running out of memory.</p>
<h5 id="convert-models">Convert Models<a hidden class="anchor" aria-hidden="true" href="#convert-models">#</a></h5>
<p>Due to the compatibility, GPTQ-for-LLaMa and AutoGPTQ doesn&rsquo;t work well for old cards and sometimes I can only find models with the old GGML model which is obsoleted. Instead of relying on
<a href="https://huggingface.co/TheBloke">TheBloke</a>, I&rsquo;d do it myself.</p>
<p>GGML to GGUF</p>
<pre tabindex="0"><code>git clone https://github.com/ggerganov/llama.cpp

cd ~/text-generation-webui/models/

wget https://huggingface.co/s3nh/llama2_13b_chat_uncensored-GGML/resolve/main/llama2_13b_chat_uncensored.ggmlv3.q5_0.bin

python3 /home/username/llama.cpp/convert-llama-ggml-to-gguf.py -i llama2_13b_chat_uncensored.ggmlv3.q5_0.bin -o llama2_13b_chat_uncensored.ggmlv3.q5_0.gguf
</code></pre><h4 id="llama-2-7b-32k-instruct">Llama-2-7B-32K-Instruct<a hidden class="anchor" aria-hidden="true" href="#llama-2-7b-32k-instruct">#</a></h4>
<p>The long context model I choose is <code>togethercomputer/Llama-2-7B-32K-Instruct</code>. It loads by <code>Transformers</code>with enable <code>use_fast</code> to function normally. <code>TheBloke/Llama-2-7B-32K-Instruct-GGUF</code> doesn&rsquo;t work for me.</p>
<p>The performance and results is really good:</p>
<pre tabindex="0"><code>Output generated in 22.14 seconds (9.53 tokens/s, 211 tokens, context 294)
Output generated in 324.32 seconds (8.53 tokens/s, 2765 tokens, context 66)
</code></pre><p>mistral-7b-instruct-v0.1.Q5_K_M.gguf</p>
<pre tabindex="0"><code>Output generated in 30.74 seconds (12.95 tokens/s, 398 tokens, context 69)
</code></pre><h3 id="tts-generation-webui">TTS Generation WebUI<a hidden class="anchor" aria-hidden="true" href="#tts-generation-webui">#</a></h3>
<p>Using the recommend installer</p>
<pre tabindex="0"><code>wget https://github.com/rsxdalv/one-click-installers-tts/archive/refs/tags/v6.0.zip
sudo chmod +x v6.0.zip
unzip v6.0.zip
cd one-click-installers-tts-6.0
./start_linux.sh
</code></pre><h4 id="bark-voice-clone">Bark Voice Clone<a hidden class="anchor" aria-hidden="true" href="#bark-voice-clone">#</a></h4>
<p>It can take a while and use the time to prepare a 15-30s voice sample.</p>
<p>When it&rsquo;s done:
Go to <code>http://ip:7860</code> → Switch to <code>Bark Voice Clone</code> tab → Upload <code>Input Audio</code> → Click <code>Generate Voice</code> → Click <code>Use as history</code> → Switch to <code>Generation (Bark)</code> tab → Click refresh button → Select the .npz sample in<code>Audio Voice</code> → Click <code>Generate</code></p>
<p>Now, it&rsquo;s time to start experimenting temperatures. <code>Save</code> the sample when satisfied.</p>
<p>Note: It will download required files while the first use so watch the output from backend between clicking.</p>
<h4 id="musicgen">MusicGen<a hidden class="anchor" aria-hidden="true" href="#musicgen">#</a></h4>
<p>Recent years, there is a emerge of AI generated music videos with obviously SD generated cover image on YouTube. I believe they are made with MusicGen.</p>
<p>So I made a few my own taste of music with prompt <code>Chiptune, KEYGEN, 8bit</code> and that sounds not bad.</p>
<h3 id="h2ogpt">h2oGPT<a hidden class="anchor" aria-hidden="true" href="#h2ogpt">#</a></h3>
<p>As a researcher, I work with a lot of ebooks and documents on a daily basis. A private offline version of <a href="https://github.com/bhaskatripathi/pdfGPT.git">pdfGPT</a> or <a href="https://www.chatpdf.com/">chatpdf</a> is extremely helpful.</p>
<p>Unfortunately, Text generation web UI is falling far behind for this specific task. By <a href="https://github.com/h2oai/h2ogpt/blob/main/docs/README_LangChain.md#what-is-h2ogpts-langchain-integration-like">comparison</a>, <a href="https://github.com/h2oai/h2ogpt">h2oGPT</a> is by far the most advanced project.</p>
<p>To install:</p>
<pre tabindex="0"><code>git clone https://github.com/h2oai/h2ogpt.git
cd h2ogpt
pip install -r requirements.txt
pip install -r reqs_optional/requirements_optional_langchain.txt
pip install -r reqs_optional/requirements_optional_gpt4all.txt
pip install pysqlite3-binary chromadb chroma-hnswlib hnswlib auto_gptq==0.4.2
python3 generate.py
</code></pre><p><a href="https://gist.github.com/defulmere/8b9695e415a44271061cc8e272f3c300?permalink_comment_id=4650539#gistcomment-4650539">Solution</a> to <code>RuntimeError: Your system has an unsupported version of sqlite3. Chroma requires sqlite3 &gt;= 3.35.0.</code></p>
<p>Added these 3 lines at the beginning:
<code>nano nano /home/user/.local/lib/python3.9/site-packages/chromadb/__init__.py</code></p>
<pre tabindex="0"><code>__import__(&#39;pysqlite3&#39;)
import sys
sys.modules[&#39;sqlite3&#39;] = sys.modules.pop(&#39;pysqlite3&#39;)
</code></pre><p>When it&rsquo;s done:
Go to <code>http://ip:7860</code> → Switch to <code>Models</code> tab → Choose Base Model <code>h2oai/h2ogpt-4096-llama2-7b-chat</code> → Click <code>Download/Load Model</code></p>
<p>Next time, launching with <code>python3 generate.py --base_model=h2oai/h2ogpt-4096-llama2-7b-chat --score_model=None --langchain_mode='UserData' --user_path=user_path</code></p>
<p>(Optional)  Launch in offline mode by <code>python3 generate.py --score_model=None --gradio_size=small --model_lock=&quot;[{'base_model': 'h2oai/h2ogpt-4096-llama2-7b-chat'}]&quot; --save_dir=save_fastup_chat --prepare_offline_level=2</code></p>
<p>Note:  Due to <code>instructor-large</code> embedding and increased context length, <code>h2oai/h2ogpt-4096-llama2-13b-chat</code> and other 13B models end up taking more than 26GB VRAM, which is out of M40&rsquo;s range. So 7B models like <code>h2ogpt-4096-llama2-7b-chat</code>, <code>ehartford/WizardLM-7B-Uncensored</code> or finetuned <code>h2ogpt-oasst1-4096-llama2-7b</code> are more appropriate (costs 14GB VRAM).</p>
<h4 id="custom-models">Custom Models<a hidden class="anchor" aria-hidden="true" href="#custom-models">#</a></h4>
<p>h2oGPT web UI provides a large selections of models from huggingface. However, I would like to have some good non-English models in addition.</p>
<p>Note:</p>
<ul>
<li>Unlike <a href="https://github.com/h2oai/h2ogpt/blob/37a8293dc762ccdbb602ffd3f17c269d029047e0/docs/FAQ.md#adding-models">FAQ</a> indicated, pre-downloading models to local and passing <code>--model_path</code> is unnecessary.</li>
<li>Fine-tuned models is more likely to avoid <a href="https://github.com/h2oai/h2ogpt/issues/910">trashy output</a></li>
<li>For text translation/interpretation/summary, instruction finetuned models are better than chat finetuned models</li>
<li>Use <code>--lora_weights=</code> to load a LoRA, <code>--use_safetensors=True</code> when load safetensors</li>
<li>Long context version is preferred</li>
<li>Recommend to add <code>--prompt_type</code> for custom models <a href="https://github.com/h2oai/h2ogpt/blob/37a8293dc762ccdbb602ffd3f17c269d029047e0/docs/FAQ.md#adding-prompt-templates">following FAQ</a> and this <a href="https://github.com/michaelthwan/llm_family_chart/">family chart</a> or this <a href="https://github.com/RUCAIBox/LLMSurvey#new-evolutionary-graph-of-llama-family">evolutionary graph</a></li>
<li>If no prompt type preset, e.g. <code>bloomz-7b1-mt</code>, load the model without passing <code>--prompt_type</code>, go to <code>Expert</code> tab and experiment prompt type in the web ui</li>
<li>It shares the same directory as other projects for HF model storage at <code>~/.cache/huggingface/hub/</code></li>
</ul>
<h4 id="non-english-models">Non-English Models<a hidden class="anchor" aria-hidden="true" href="#non-english-models">#</a></h4>
<p>Since vanilla Llama2 does not work well responding non-English languages, I have tested a list of Chinese/multilingual LLMs and found some useful result. Others may do the same for their preferred language:</p>
<ul>
<li><code>LinkSoul/Chinese-Llama-2-7b</code>+<code>zigge106/LinkSoul-Chinese-Llama-2-7b-200wechat-chatgpt-best</code>: Chats ok but extremely bad performance working with context, with or without LoRA, unable to complete any test</li>
<li><code>THUDM/chatglm2-6b</code>or <code>Echolist-yixuan/chatglm2-6b-qlora</code>: Chats ok but only generates garbage for context, unable to complete any test</li>
<li><code>ziqingyang/chinese-alpaca-2-7b</code>+<code>ziqingyang/chinese-alpaca-2-lora-7b</code>: good performance, no good prompt type with or without LoRA, language disturbance, only hallucinates if not generates garbage, typical Artificial Imbecility</li>
<li><code>baichuan-inc/Baichuan2-7B-Chat</code>: prompt type <code>mptinstruct</code>,  <code>openai_chat</code>, <code>wizard2</code> and etc., good performance, good intelligence but hallucinates, normal accuracy</li>
<li><code>Linly-AI/Chinese-LLaMA-2-7B-hf</code>: prompt type <code>llama2</code> doesn&rsquo;t work, instead use <code>instruct</code>,  <code>quality</code>, <code>gptj</code> and etc., good performance and okay intelligence, bad accuracy</li>
<li><code>OpenBuddy/openbuddy-openllama-7b-v12-bf16</code>: prompt type <code>openai_chat</code>,  <code>quality</code>, <code>gptj</code> and etc., good performance, normal intelligence, normal accuracy, recommend</li>
<li><code>FreedomIntelligence/phoenix-inst-chat-7b</code>: prompt type  <code>guanaco</code>,  <code>open_assistant</code>, <code>wizard_lm</code> and etc., good performance, normal intelligence, normal accuracy, recommend</li>
<li><code>BelleGroup/BELLE-7B-2M</code>: prompt type <code>guanaco</code>,  <code>instruct</code>, <code>beluga</code> and etc., normal performance, bad intelligence, bad accuracy</li>
<li><code>Qwen/Qwen-7B-Chat</code>: prompt type <code>wizard_lm</code>,  <code>quality</code>, <code>wizard3</code> and etc., good performance, great intelligence but overly creative, censored, bad accuracy, worth a try</li>
<li><code>xverse/XVERSE-7B-Chat</code>: prompt type <code>one_shot</code>,  <code>mptinstruct</code>, <code>gptj</code> and etc., normal performance, good intelligence but overly creative, bad accuracy, recommend</li>
<li><code>internlm/internlm-chat-7b</code>:low performance, no good prompt type, strong language disturbance, resists against language preset, only hallucinates if not generates garbage, typical Artificial Imbecility</li>
<li><code>PengQu/Llama-2-7b-vicuna-Chinese</code>: Prompt type <code>instruct_vicuna</code>,  <code>open_assistant</code>, <code>instruct_vicuna2</code> and etc., low performance, good intelligence but creative, hallucinates, normal accuracy, does not avoid violence</li>
</ul>
<p>The average accuracy is not satisfying probably due to 7B size. Tinkering configurations in <code>Expert</code> tab may help. Further custom training may be required as well.</p>
<p>Modify <code>System/Query/Summary (Pre-)Prompt</code> under <code>Expert</code> tab and <code>Prompt (or Custom)</code> in web ui or passing <code>--prompt_dict</code> accordingly to fit non-English language preference:</p>
<ul>
<li><code>System Prompt</code>: <code>用中文回答问题或响应指令</code></li>
<li><code>Query Pre-Prompt</code>: <code>请注意并记住下面的信息，这将有助于在情境结束后回答问题或遵循命令。</code></li>
<li><code>Query Prompt</code>: <code>根据上文提供的文档来源中的信息，</code></li>
<li><code>Summary Pre-Prompt</code>: <code>为了撰写简明扼要的单段或项目符号列表摘要，请注意以下文本</code></li>
<li><code>Summary Prompt</code>: <code>仅利用上述文档来源中的信息，写一个简明扼要的关键结果总结（最好使用项目符号）：</code></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://techshinobi.org/tags/vostro/">Vostro</a></li>
      <li><a href="https://techshinobi.org/tags/cheapskate/">Cheapskate</a></li>
      <li><a href="https://techshinobi.org/tags/optiplex/">Optiplex</a></li>
      <li><a href="https://techshinobi.org/tags/opensource/">Opensource</a></li>
      <li><a href="https://techshinobi.org/tags/philosophy/">Philosophy</a></li>
      <li><a href="https://techshinobi.org/tags/api/">api</a></li>
      <li><a href="https://techshinobi.org/tags/richard-m.-stallma/">Richard M. Stallma</a></li>
      <li><a href="https://techshinobi.org/tags/debian/">Debian</a></li>
      <li><a href="https://techshinobi.org/tags/nvidia-tesla-m40/">NVIDIA Tesla M40</a></li>
      <li><a href="https://techshinobi.org/tags/swap/">swap</a></li>
      <li><a href="https://techshinobi.org/tags/hardware/">Hardware</a></li>
      <li><a href="https://techshinobi.org/tags/thermal-mod/">Thermal Mod</a></li>
      <li><a href="https://techshinobi.org/tags/bios/">BIOS</a></li>
      <li><a href="https://techshinobi.org/tags/subs-ai-whisper/">Subs AI Whisper</a></li>
      <li><a href="https://techshinobi.org/tags/text-generation-web-ui/">Text generation web UI</a></li>
      <li><a href="https://techshinobi.org/tags/llama2-uncensored/">Llama2 uncensored</a></li>
      <li><a href="https://techshinobi.org/tags/tts-generation-webui/">TTS Generation WebUI</a></li>
      <li><a href="https://techshinobi.org/tags/voice-clone/">Voice Clone</a></li>
      <li><a href="https://techshinobi.org/tags/h2ogpt/">h2oGPT</a></li>
      <li><a href="https://techshinobi.org/tags/chatpdf/">chatpdf</a></li>
      <li><a href="https://techshinobi.org/tags/chinese-llms/">Chinese LLMs</a></li>
      <li><a href="https://techshinobi.org/tags/artificial-imbecility/">Artificial Imbecility</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://techshinobi.org/posts/recoverluks/">
    <span class="title">Next Page »</span>
    <br>
    <span>Recovering from Data Loss due to LUKS Failure</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Cheapskate&#39;s Homebrew AI Lab on twitter"
        href="https://twitter.com/intent/tweet/?text=Cheapskate%27s%20Homebrew%20AI%20Lab&amp;url=https%3a%2f%2ftechshinobi.org%2fposts%2fcheapai%2f&amp;hashtags=Vostro%2cCheapskate%2cOptiplex%2cOpensource%2cPhilosophy%2cAPI%2cRichardM.Stallma%2cDebian%2cNVIDIATeslaM40%2cswap%2cHardware%2cThermalMod%2cBIOS%2cSubsAIWhisper%2cTextgenerationwebUI%2cLlama2uncensored%2cTTSGenerationWebUI%2cVoiceClone%2ch2oGPT%2cchatpdf%2cChineseLLMs%2cArtificialImbecility">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Cheapskate&#39;s Homebrew AI Lab on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftechshinobi.org%2fposts%2fcheapai%2f&amp;title=Cheapskate%27s%20Homebrew%20AI%20Lab&amp;summary=Cheapskate%27s%20Homebrew%20AI%20Lab&amp;source=https%3a%2f%2ftechshinobi.org%2fposts%2fcheapai%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Cheapskate&#39;s Homebrew AI Lab on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ftechshinobi.org%2fposts%2fcheapai%2f&title=Cheapskate%27s%20Homebrew%20AI%20Lab">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Cheapskate&#39;s Homebrew AI Lab on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftechshinobi.org%2fposts%2fcheapai%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Cheapskate&#39;s Homebrew AI Lab on whatsapp"
        href="https://api.whatsapp.com/send?text=Cheapskate%27s%20Homebrew%20AI%20Lab%20-%20https%3a%2f%2ftechshinobi.org%2fposts%2fcheapai%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Cheapskate&#39;s Homebrew AI Lab on telegram"
        href="https://telegram.me/share/url?text=Cheapskate%27s%20Homebrew%20AI%20Lab&amp;url=https%3a%2f%2ftechshinobi.org%2fposts%2fcheapai%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://techshinobi.org/">Tech Shinobi</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
