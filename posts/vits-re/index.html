<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Revisiting Voice Cloning with GPT-SoVITS and so on | Tech Shinobi</title>
<meta name="keywords" content="CosyVoice, GPT-SoVITS, fish-speech, Opensource, Retrieval-based-Voice-Conversion, RVC, Seed-VC, Voice Clone, Model Training, Inference, realtime voice changing, Diffusion, Text-to-Speech, whisper">
<meta name="description" content="Forewords
My last article on voice cloning is more than a year ago, and here we are again for adopting some latest advancement.
Refering to some Chinese source such as this blog and this video, I was attempting to adopt new tools for my audio book service, such as CosyVoice, F5-TTS, GPT-SoVITS, and fish-speech.
But before we start, I recommend to:
Install miniconda for dependency sanity
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp; sudo chmod &#43;x Miniconda3-latest-Linux-x86_64.sh &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh
Setup PyTorch environment as needed and confirm with python -m torch.utils.collect_env">
<meta name="author" content="Jun">
<link rel="canonical" href="https://techshinobi.org/posts/vits-re/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.93f625d739f1d6a5c6f20c146bc6a8d26b233492b34b2220c54b12fd46a04ded.css" integrity="sha256-k/Yl1znx1qXG8gwUa8ao0msjNJKzSyIgxUsS/UagTe0=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://techshinobi.org/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://techshinobi.org/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://techshinobi.org/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://techshinobi.org/apple-touch-icon.png">
<link rel="mask-icon" href="https://techshinobi.org/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://techshinobi.org/posts/vits-re/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:url" content="https://techshinobi.org/posts/vits-re/">
  <meta property="og:site_name" content="Tech Shinobi">
  <meta property="og:title" content="Revisiting Voice Cloning with GPT-SoVITS and so on">
  <meta property="og:description" content="Forewords
My last article on voice cloning is more than a year ago, and here we are again for adopting some latest advancement.
Refering to some Chinese source such as this blog and this video, I was attempting to adopt new tools for my audio book service, such as CosyVoice, F5-TTS, GPT-SoVITS, and fish-speech.
But before we start, I recommend to:
Install miniconda for dependency sanity
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp; sudo chmod &#43;x Miniconda3-latest-Linux-x86_64.sh &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh Setup PyTorch environment as needed and confirm with python -m torch.utils.collect_env">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-06-11T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-06-11T00:00:00+00:00">
    <meta property="article:tag" content="CosyVoice">
    <meta property="article:tag" content="GPT-SoVITS">
    <meta property="article:tag" content="Fish-Speech">
    <meta property="article:tag" content="Opensource">
    <meta property="article:tag" content="Retrieval-Based-Voice-Conversion">
    <meta property="article:tag" content="RVC">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Revisiting Voice Cloning with GPT-SoVITS and so on">
<meta name="twitter:description" content="Forewords
My last article on voice cloning is more than a year ago, and here we are again for adopting some latest advancement.
Refering to some Chinese source such as this blog and this video, I was attempting to adopt new tools for my audio book service, such as CosyVoice, F5-TTS, GPT-SoVITS, and fish-speech.
But before we start, I recommend to:
Install miniconda for dependency sanity
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp; sudo chmod &#43;x Miniconda3-latest-Linux-x86_64.sh &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh
Setup PyTorch environment as needed and confirm with python -m torch.utils.collect_env">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://techshinobi.org/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Revisiting Voice Cloning with GPT-SoVITS and so on",
      "item": "https://techshinobi.org/posts/vits-re/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Revisiting Voice Cloning with GPT-SoVITS and so on",
  "name": "Revisiting Voice Cloning with GPT-SoVITS and so on",
  "description": "Forewords\nMy last article on voice cloning is more than a year ago, and here we are again for adopting some latest advancement.\nRefering to some Chinese source such as this blog and this video, I was attempting to adopt new tools for my audio book service, such as CosyVoice, F5-TTS, GPT-SoVITS, and fish-speech.\nBut before we start, I recommend to:\nInstall miniconda for dependency sanity\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \u0026amp;\u0026amp; sudo chmod +x Miniconda3-latest-Linux-x86_64.sh \u0026amp;\u0026amp; bash Miniconda3-latest-Linux-x86_64.sh Setup PyTorch environment as needed and confirm with python -m torch.utils.collect_env\n",
  "keywords": [
    "CosyVoice", "GPT-SoVITS", "fish-speech", "Opensource", "Retrieval-based-Voice-Conversion", "RVC", "Seed-VC", "Voice Clone", "Model Training", "Inference", "realtime voice changing", "Diffusion", "Text-to-Speech", "whisper"
  ],
  "articleBody": "Forewords\nMy last article on voice cloning is more than a year ago, and here we are again for adopting some latest advancement.\nRefering to some Chinese source such as this blog and this video, I was attempting to adopt new tools for my audio book service, such as CosyVoice, F5-TTS, GPT-SoVITS, and fish-speech.\nBut before we start, I recommend to:\nInstall miniconda for dependency sanity\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \u0026\u0026 sudo chmod +x Miniconda3-latest-Linux-x86_64.sh \u0026\u0026 bash Miniconda3-latest-Linux-x86_64.sh Setup PyTorch environment as needed and confirm with python -m torch.utils.collect_env\nInstall nvtop if preferred by sudo apt install nvtop\nGPT-SoVITS This project is made by same group of people from so-vits-svc. The model’s quality improved greatly from v2 to v4. Although when doing long text tts, errors are unavoidable, it is good enough for my use case.\nBy the time of writing this artile, they released a new version 20250606v2pro which may have some differences since I was using version 20250422v4.\nBut, you can always using their “windows package” which packed with all models and works on Linux servers despite its name, so that is intended to provide a more user friendly “one-click” experience.\nInstall on Linux\ngit clone https://github.com/RVC-Boss/GPT-SoVITS.git \u0026\u0026 cd GPT-SoVITS conda create -n GPTSoVits python=3.10 conda activate GPTSoVits #auto install script bash install.sh --source HF --download-uvr5 #(optional) manual install pip install -r extra-req.txt --no-deps pip install -r requirements.txt Install FFmpeg and other deps\nsudo apt install ffmpeg sudo apt install libsox-dev #(optional for troubleshooting) conda install -c conda-forge 'ffmpeg\u003c7' pip install -U gradio python -m nltk.downloader averaged_perceptron_tagger_eng (optional) Download Pretrained ASR Models for Chinese\ngit lfs install cd tools/asr/models/ git clone https://www.modelscope.cn/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch.git tools/asr/models/speech_fsmn_vad_zh-cn-16k-common-pytorch git clone https://www.modelscope.cn/iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git tools/asr/models/punc_ct-transformer_zh-cn-common-vocab272727-pytorch git clone https://www.modelscope.cn/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git tools/asr/models/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch After all done, run GRADIO_SHARE=0 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python -m webui.py to start server then access via http://ip:9874/\n0-Fetch dataset preparation-for-vocal-recording\nI used Audacity instead of UVR5 because the recording is clean and clear.\nUse the webui built-in audio slicer to slice the new recording.wav and put old recordings (if any) all together under output/slicer_opt\nUse built-in batch ASR tool with faster whisper, since I’m doing multilingual model this time.\nThese issues are related exclusively with old GPU architecture. No worries for new GPU (30x0/40x0) users.\nTroubleshooting 1 RuntimeError: parallel_for failed: cudaErrorNoKernelImageForDevice: no kernel image is available for execution on the device.\npip uninstall -y ctranslate2 pip install ctranslate2==3.24.0 Troubleshooting 2 'iwrk': array([], dfitpack_int), 'u': array([], float),\npip uninstall numpy scipy pip install numba==0.60.0 numpy==1.26.4 scipy After transcription finished, use built-in labeling tool(subfix) to remove bad samples. If the web page doesn’t pop-up, use ip:9871 manually. Choose Audio and Delete Audio, Save File when finished.\n1-GPT-SOVITS-TTS 1A-Dataset formatting Fill up the empty fields and click Set One-Click Formatting:\n#Text labelling file /home/username/GPT-SoVITS/output/asr_opt/slicer_opt.list #Audio dataset folder output/slicer_opt 1B-Fine-tuned training 1Ba-SoVITS training Use batch size at 1, total epoch at 5 and save_every_epoch at 1.\n1Bb-GPT training For this part, my batch size is 6 with DPO enabled. Total traning epochs should be around 5-15, adjust Save frequency based on needs.\nTroubleshooting for old GPUs Error: cuFFT doesn't support signals of half type with compute capability less than SM_53, but the device containing input half tensor only has SM_52.\nFix1: Eidt webui.py, add a new line after from multiprocessing import cpu_count with is_half = False Fix2: Edit GPT_SoVITS/s2_train.py, add hps.train.fp16_run = False at beginning (among with torch.backends.cudnn.benchmark = False)\n1C-inference click refreshing model paths and select model in both lists\nCheck Enable Parallel Inference Version then open TTS Inference WebUI, this need a while to load, manually access ip:9872 if needed.\nTroubleshooting for ValueError: Due to a serious vulnerability issue in torch.load fix by pip install transformers==4.43\nInference Settings:\ne3.ckpt e15.pth Primary Reference Audio with Text and multiple reference audio Slice by every punct top_k 5 top_p 1 temperature 0.9 Repetition Penalty 2 speed_factor 1.3 Keep everything else default.\nTo find the best GPT weight, parameters and random seed, first inferencing on a large block of text and pick up a few problematic sentences for next inference. Then, adjust GPT weight and parameters to make the problem go away while on a fixed seed number. Once the best GPT weight and parameters are found, fix them then play with difference seed number to refine the final result. Take note on the parameters when inference gets perfect for future use.\nFish Speech Fish-speech is contributed by the same people from Bert-vits2 which I used for a long time.\nFollowing their official docs to install version 1.4 (unfortunately, v1.5 has problem of sound quality while finetuning)\ngit clone --branch v1.4.3 https://github.com/fishaudio/fish-speech.git \u0026\u0026 cd fish-speech conda create -n fish-speech python=3.10 conda activate fish-speech pip3 install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 apt install libsox-dev ffmpeg apt install build-essential \\ cmake \\ libasound-dev \\ portaudio19-dev \\ libportaudio2 \\ libportaudiocpp0 pip3 install -e . Download required models\nhuggingface-cli download fishaudio/fish-speech-1.4 --local-dir checkpoints/fish-speech-1.4 Prepare dataset\nmkdir data cp -r /home/username/GPT-SoVITS/output/slicer_opt data/ python tools/whisper_asr.py --audio-dir data/slicer_opt --save-dir data/slicer_opt --compute-type float32 python tools/vqgan/extract_vq.py data \\ --num-workers 1 --batch-size 16 \\ --config-name \"firefly_gan_vq\" \\ --checkpoint-path \"checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\" python tools/llama/build_dataset.py \\ --input \"data\" \\ --output \"data/protos\" \\ --text-extension .lab \\ --num-workers 16 Edit parameters by nano fish_speech/configs/text2semantic_finetune.yaml and start training\npython fish_speech/train.py --config-name text2semantic_finetune \\ project=$project \\ +lora@model.model.lora_config=r_8_alpha_16 Note:\nThe default numbers are too high for my setup, so both num_workers and batch_size needs to be lowered according to CPU cores and VRAM. For the first run, I set max_steps: 10000 and val_check_interval: 1000 to have 5 models that have lower steps with some diversity . Things like lr, weight_decay and num_warmup_steps can be further adjusted accroding to this article. My setting is lr: 1e-5, weight_decay: 1e-6, num_warmup_steps: 500. To check to training metrics such as loss curve, run tensorboard --logdir fish-speech/results/tensorboard/version_xx/ and access localhost:6006 via browser. Determine overfitting with the graph AND actuall hear to the inference result for each checkpoint. At first I found out overfitting starts around 5000 step. Then a a second training for 5000 steps and find the best result is step_000004000.ckpt. Training requires a newer GPU with bf16 and no workaround so far. When training a model for inferencing on an older GPU, use precision: 32-true in fish_speech/configs/text2semantic_finetune.yaml and result += (self.lora_dropout(x).to(torch.float32) @ self.lora_A.to(torch.float32).transpose(0, 1) @ self.lora_B.to(torch.float32).transpose(0, 1)) * self.scaling.to(torch.float32) in/home/username/miniconda3/envs/fish-speech/lib/python3.10/site-packages/loralib/layers.py. Training would take many hours on weak GPU. After finished, convert the LoRA weights\npython tools/llama/merge_lora.py \\ --lora-config r_8_alpha_16 \\ --base-weight checkpoints/fish-speech-1.4 \\ --lora-weight results/$project/checkpoints/step_000005000.ckpt \\ --output checkpoints/fish-speech-1.4-yth-lora/ Generate prompt and semantic tokens\npython tools/vqgan/inference.py \\ -i \"1.wav\" \\ --checkpoint-path \"checkpoints/fish-speech-1.4/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\" Troubleshooting for old GPU 1 Unable to load any of {libcudnn_ops.so.9.1.0, libcudnn_ops.so.9.1, libcudnn_ops.so.9, libcudnn_ops.so}\npip uninstall -y ctranslate2 pip install ctranslate2==3.24.0 Troubleshooting for old GPU 2 ImportError: cannot import name 'is_callable_allowed' from partially initialized module 'torch._dynamo.trace_rules'\nconda install pytorch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 pytorch-cuda=11.8 -c pytorch -c nvidia Make it accessible from LAN nano tools/run_webui.py\napp.launch(server_name=\"0.0.0.0\", server_port=7860, show_api=True) Change the --llama-checkpoint-path to the newly trained LoRA, and start WebUI (added --half for my old GPU to avoid bf16 error)\nGRADIO_SHARE=0 GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=7860 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python -m tools.webui \\ --llama-checkpoint-path \"checkpoints/fish-speech-1.4-yth-lora\" \\ --decoder-checkpoint-path \"checkpoints/fish-speech-1.4/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\" \\ --decoder-config-name firefly_gan_vq \\ --half Parameters for inferencing Enable Reference Audio check Text Normalization Interative Prompt Lenth 200 Top-P 0.8 Temperature 0.7 Repetition Penalty 1.5 Set Seed\nNote:\nhigher number to compensate overfitted model, lower number for underfitted model. certain punctuation or tab space may trigger noise generation. Text normalization suppose to address these issue but sometimes I still need to find \u0026 replace. However, a bug Negative code found occurs quite frequent while inferencing without solution by now. Give up.\nCosyVoice CosyVoice is one from the FunAudioLLM toolkits, which developed by the same team from Alibaba’s Qwen I use a lot.\nInstall\ngit clone --recursive https://github.com/FunAudioLLM/CosyVoice.git \u0026\u0026 cd CosyVoice git submodule update --init --recursive conda create -n cosyvoice -y python=3.10 conda activate cosyvoice conda install -y -c conda-forge pynini==2.1.5 sudo apt-get install sox libsox-dev -y pip install -r requirements.txt Download Pretrained Models\ngit lfs install mkdir -p pretrained_models git clone https://huggingface.co/FunAudioLLM/CosyVoice2-0.5B pretrained_models/CosyVoice2-0.5B git clone https://huggingface.co/FunAudioLLM/CosyVoice-300M pretrained_models/CosyVoice-300M git clone https://huggingface.co/FunAudioLLM/CosyVoice-300M-SFT pretrained_models/CosyVoice-300M-SFT git clone https://huggingface.co/FunAudioLLM/CosyVoice-300M-Instruct pretrained_models/CosyVoice-300M-Instruct Run with\nGRADIO_SHARE=0 GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=7860 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python -m webui.py --port 50000 --model_dir pretrained_models/CosyVoice-300M Troubleshooting “GLIBCXX_3.4.29’ not found” with this\nstrings /usr/lib/x86_64-linux-gnu/libstdc++.so.6 | grep GLIBCXX strings $CONDA_PREFIX/lib/libstdc++.so.6 | grep GLIBCXX nano ~/.bashrc export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH find / -name \"libstdc++.so*\" rm /home/username/anaconda3/lib/python3.11/site-packages/../../libstdc++.so.6 ln -s /home/username/text-generation-webui/installer_files/env/lib/libstdc++.so.6.0.29 /home/username/anaconda3/lib/python3.11/site-packages/../../libstdc++.so.6 It ends up working fine but not as good as GPT-SoVITS. Hope their 3.0 version can pump it up.\nVoice Conversion Both RVC and Seed-VC are intended to replace my good old so-vits-svc instance.\nRetrieval-based-Voice-Conversion Install\ngit clone https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI \u0026\u0026 cd Retrieval-based-Voice-Conversion-WebUI conda create -n rvc -y python=3.8 conda activate rvc pip install torch torchvision torchaudio pip install pip==24.0 pip install -r requirements.txt python tools/download_models.py sudo apt install ffmpeg wget https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/rmvpe.pt Run with python infer-web.py, fill up following then click buttons step by step with default settings:\nEnter the experiment name:/path/to/raw/ Troubleshooting “enabled=hps.train.fp16_run”\nSeed-VC Install\ngit clone https://github.com/Plachtaa/seed-vc \u0026\u0026 cd Retrieval-based-Voice-Conversion-WebUI conda create -n seedvc -y python=3.10 conda activate seedvc pip install -r requirements.txt GRADIO_SHARE=0 GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=7860 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python app.py --enable-v1 --enable-v2 Settings\n#V2 Diffusion Steps: 100 Length Adjust: 1 Intelligibility CFG Rate: 0 Similarity CFG Rate: 1 Top-p: 1 Temperature: 1 Repetition Penalty: 2 convert style/emotion/accent: check #V1 Diffusion Steps: 100 Length Adjust: 1 Inference CFG Rate: 1 Use F0 conditioned model: check Auto F0 adjust: check Pitch shift: 0 Training\npython train.py --config /home/username/seed-vc/configs/presets/config_dit_mel_seed_uvit_whisper_base_f0_44k.yml --dataset-dir /home/username/GPT-SoVITS-v4/output/slicer_opt --run-name username --batch-size 6 --max-steps 10000 --max-epochs 10000 --save-every 1000 --num-workers 1 accelerate launch train_v2.py --dataset-dir /home/username/GPT-SoVITS-v4/output/slicer_opt --run-name username-v2 --batch-size 6 --max-steps 2000 --max-epochs 2000 --save-every 200 --num-workers 0 --train-cfm Using checkpoints\n#Voice Conversion Web UI GRADIO_SHARE=0 GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=7860 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python app_vc.py --checkpoint ./runs/test01/ft_model.pth --config ./configs/presets/config_dit_mel_seed_uvit_whisper_base_f0_44k.yml --fp16 False #Singing Voice Conversion Web UI GRADIO_SHARE=0 GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=7860 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python app_svc.py --checkpoint ./runs/username/DiT_epoch_00029_step_08000.pth --config ./configs/presets/config_dit_mel_seed_uvit_whisper_base_f0_44k.yml --fp16 False #V2 model Web UI GRADIO_SHARE=0 GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=7860 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python app_vc_v2.py --cfm-checkpoint-path runs/Satine-V2/CFM_epoch_00000_step_00600.pth It turned out V1 model with Singing Voice Conversion Web UI app_svc.py performs the best.\n",
  "wordCount" : "1653",
  "inLanguage": "en",
  "datePublished": "2025-06-11T00:00:00Z",
  "dateModified": "2025-06-11T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Jun"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://techshinobi.org/posts/vits-re/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Tech Shinobi",
    "logo": {
      "@type": "ImageObject",
      "url": "https://techshinobi.org/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://techshinobi.org/" accesskey="h" title="Tech Shinobi (Alt + H)">Tech Shinobi</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://techshinobi.org/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://techshinobi.org/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://techshinobi.org/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://techshinobi.org/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://techshinobi.org/">Home</a>&nbsp;»&nbsp;<a href="https://techshinobi.org/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Revisiting Voice Cloning with GPT-SoVITS and so on
    </h1>
    <div class="post-meta"><span title='2025-06-11 00:00:00 +0000 UTC'>June 11, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Jun

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#gpt-sovits" aria-label="GPT-SoVITS">GPT-SoVITS</a><ul>
                        
                <li>
                    <a href="#0-fetch-dataset" aria-label="0-Fetch dataset">0-Fetch dataset</a></li>
                <li>
                    <a href="#1-gpt-sovits-tts" aria-label="1-GPT-SOVITS-TTS">1-GPT-SOVITS-TTS</a><ul>
                        
                <li>
                    <a href="#1a-dataset-formatting" aria-label="1A-Dataset formatting">1A-Dataset formatting</a></li>
                <li>
                    <a href="#1b-fine-tuned-training" aria-label="1B-Fine-tuned training">1B-Fine-tuned training</a></li>
                <li>
                    <a href="#1c-inference" aria-label="1C-inference">1C-inference</a></li></ul>
                </li></ul>
                </li></ul>
                    
                <li>
                    <a href="#fish-speech" aria-label="Fish Speech">Fish Speech</a><ul>
                        
                <li>
                    <a href="#cosyvoice" aria-label="CosyVoice">CosyVoice</a></li></ul>
                </li>
                <li>
                    <a href="#voice-conversion" aria-label="Voice Conversion">Voice Conversion</a><ul>
                        
                <li>
                    <a href="#retrieval-based-voice-conversion" aria-label="Retrieval-based-Voice-Conversion">Retrieval-based-Voice-Conversion</a></li>
                <li>
                    <a href="#seed-vc" aria-label="Seed-VC">Seed-VC</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><strong>Forewords</strong></p>
<p><a href="https://techshinobi.org/posts/voice-vits/">My last article</a> on voice cloning is more than a year ago, and here we are again for adopting some latest advancement.</p>
<p>Refering to some Chinese source such as <a href="https://www.tangshuang.net/9063.html">this blog</a> and <a href="https://www.bilibili.com/video/BV1hukmYhEJX">this video</a>, I was attempting to adopt new tools for my audio book service, such as <a href="%5Bhttps://github.com/FunAudioLLM/CosyVoice">CosyVoice</a>, <a href="https://github.com/SWivid/F5-TTS/">F5-TTS</a>, <a href="https://github.com/RVC-Boss/GPT-SoVITS">GPT-SoVITS</a>, and <a href="https://github.com/fishaudio/fish-speech">fish-speech</a>.</p>
<p>But before we start, I recommend to:</p>
<p>Install miniconda for dependency sanity</p>
<pre tabindex="0"><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp; sudo chmod +x Miniconda3-latest-Linux-x86_64.sh &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh
</code></pre><p>Setup <a href="https://pytorch.org/get-started/previous-versions/#linux-and-windows-2">PyTorch environment</a> as needed and confirm with <code>python -m torch.utils.collect_env</code></p>
<p>Install <code>nvtop</code> if preferred by <code>sudo apt install nvtop</code></p>
<h3 id="gpt-sovits">GPT-SoVITS<a hidden class="anchor" aria-hidden="true" href="#gpt-sovits">#</a></h3>
<p><a href="https://github.com/RVC-Boss/GPT-SoVITS">This project</a> is made by same group of people from <a href="https://techshinobi.org/posts/voice-vits/#so-vits-svc">so-vits-svc</a>. The model&rsquo;s quality improved greatly from v2 to v4. Although when doing long text tts, errors are unavoidable, it is good enough for my use case.</p>
<p>By the time of writing this artile, they released a new version <code>20250606v2pro</code> which may have some differences since I was using version <code>20250422v4</code>.</p>
<p>But, you can always using their <a href="https://huggingface.co/lj1995/GPT-SoVITS-windows-package/tree/main">&ldquo;windows package&rdquo;</a> which packed with all models and works on Linux servers despite its name, so that is intended to provide a more user friendly &ldquo;one-click&rdquo; experience.</p>
<p>Install on Linux</p>
<pre tabindex="0"><code>git clone https://github.com/RVC-Boss/GPT-SoVITS.git &amp;&amp; cd GPT-SoVITS
conda create -n GPTSoVits python=3.10
conda activate GPTSoVits
#auto install script
bash install.sh --source HF --download-uvr5
#(optional) manual install
pip install -r extra-req.txt --no-deps
pip install -r requirements.txt
</code></pre><p>Install FFmpeg and other deps</p>
<pre tabindex="0"><code>sudo apt install ffmpeg
sudo apt install libsox-dev

#(optional for troubleshooting)
conda install -c conda-forge &#39;ffmpeg&lt;7&#39;
pip install -U gradio
python -m nltk.downloader averaged_perceptron_tagger_eng
</code></pre><p>(optional) Download Pretrained ASR Models for Chinese</p>
<pre tabindex="0"><code>git lfs install
cd tools/asr/models/
git clone https://www.modelscope.cn/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch.git tools/asr/models/speech_fsmn_vad_zh-cn-16k-common-pytorch
git clone https://www.modelscope.cn/iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git tools/asr/models/punc_ct-transformer_zh-cn-common-vocab272727-pytorch
git clone https://www.modelscope.cn/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git tools/asr/models/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch
</code></pre><p>After all done, run <code>GRADIO_SHARE=0 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python -m webui.py</code> to start server then access via <code>http://ip:9874/</code></p>
<h4 id="0-fetch-dataset">0-Fetch dataset<a hidden class="anchor" aria-hidden="true" href="#0-fetch-dataset">#</a></h4>
<p><a href="https://techshinobi.org/posts/voice-vits/#preparation-for-vocal-recording">preparation-for-vocal-recording</a></p>
<p>I used Audacity instead of UVR5 because the recording is clean and clear.</p>
<p>Use the webui built-in audio slicer to slice the new recording.wav and put old recordings (if any) all together under <code>output/slicer_opt</code></p>
<p>Use built-in batch ASR tool with faster whisper, since I&rsquo;m doing multilingual model this time.</p>
<p>These issues are related exclusively with old GPU architecture. No worries for new GPU (30x0/40x0) users.</p>
<p><a href="https://github.com/jianchang512/stt/issues/51">Troubleshooting 1</a> <code>RuntimeError: parallel_for failed: cudaErrorNoKernelImageForDevice: no kernel image is available for execution on the device.</code></p>
<pre tabindex="0"><code>pip uninstall -y ctranslate2
pip install ctranslate2==3.24.0
</code></pre><p><a href="https://github.com/comfyanonymous/ComfyUI/issues/5895">Troubleshooting 2</a> <code>'iwrk': array([], dfitpack_int), 'u': array([], float),</code></p>
<pre tabindex="0"><code>pip uninstall numpy scipy
pip install numba==0.60.0 numpy==1.26.4 scipy
</code></pre><p>After transcription finished, use built-in labeling tool(subfix) to remove bad samples. If the web page doesn&rsquo;t pop-up, use <code>ip:9871</code> manually. Choose Audio and Delete Audio, Save File when finished.</p>
<h4 id="1-gpt-sovits-tts">1-GPT-SOVITS-TTS<a hidden class="anchor" aria-hidden="true" href="#1-gpt-sovits-tts">#</a></h4>
<h5 id="1a-dataset-formatting">1A-Dataset formatting<a hidden class="anchor" aria-hidden="true" href="#1a-dataset-formatting">#</a></h5>
<p>Fill up the empty fields and click <code>Set One-Click Formatting</code>:</p>
<pre tabindex="0"><code>#Text labelling file
/home/username/GPT-SoVITS/output/asr_opt/slicer_opt.list

#Audio dataset folder
output/slicer_opt
</code></pre><h5 id="1b-fine-tuned-training">1B-Fine-tuned training<a hidden class="anchor" aria-hidden="true" href="#1b-fine-tuned-training">#</a></h5>
<p>1Ba-SoVITS training
Use <code>batch size</code> at <code>1</code>, <code>total epoch</code> at <code>5</code> and <code>save_every_epoch</code> at <code>1</code>.</p>
<p>1Bb-GPT training
For this part, my <code>batch size</code> is <code>6</code> with <code>DPO enabled</code>. Total traning epochs should be around 5-15, adjust Save frequency based on needs.</p>
<p>Troubleshooting for old GPUs
Error: <code>cuFFT doesn't support signals of half type with compute capability less than SM_53,  but the device containing input half tensor only has SM_52</code>.</p>
<p>Fix1: Eidt <code>webui.py</code>, add a new line after <code>from multiprocessing import cpu_count</code> with <code>is_half = False</code>
Fix2: Edit <code>GPT_SoVITS/s2_train.py</code>, add <code>hps.train.fp16_run = False</code> at beginning (among with <code>torch.backends.cudnn.benchmark = False</code>)</p>
<h5 id="1c-inference">1C-inference<a hidden class="anchor" aria-hidden="true" href="#1c-inference">#</a></h5>
<p>click <code>refreshing model paths</code> and select model in both lists</p>
<p>Check <code>Enable Parallel Inference Version</code> then <code>open TTS Inference WebUI</code>, this need a while to load, manually access <code>ip:9872</code> if needed.</p>
<p>Troubleshooting for <code>ValueError: Due to a serious vulnerability issue in torch.load</code> fix by <code>pip install transformers==4.43</code></p>
<p>Inference Settings:</p>
<ul>
<li>e3.ckpt</li>
<li>e15.pth</li>
<li>Primary Reference Audio with Text and multiple reference audio</li>
<li>Slice by every punct</li>
<li>top_k 5</li>
<li>top_p 1</li>
<li>temperature 0.9</li>
<li>Repetition Penalty 2</li>
<li>speed_factor 1.3</li>
</ul>
<p>Keep everything else default.</p>
<p>To find the best GPT weight, parameters and random seed, first inferencing on a large block of text and pick up a few problematic sentences for next inference. Then, adjust GPT weight and parameters to make the problem go away while on a fixed seed number.  Once the best GPT weight and parameters are found, fix them then play with difference seed number to refine the final result. Take note on the parameters when inference gets perfect for future use.</p>
<h2 id="fish-speech">Fish Speech<a hidden class="anchor" aria-hidden="true" href="#fish-speech">#</a></h2>
<p><a href="https://github.com/fishaudio/fish-speech">Fish-speech</a> is contributed by the same people from <a href="https://techshinobi.org/posts/voice-vits/#bert-vits2-v23">Bert-vits2</a> which I used for a long time.</p>
<p>Following their <a href="https://web.archive.org/web/20241006183809/https://speech.fish.audio/">official docs</a> to install version 1.4 (unfortunately, v1.5 has problem of sound quality while finetuning)</p>
<pre tabindex="0"><code>git clone --branch v1.4.3 https://github.com/fishaudio/fish-speech.git &amp;&amp; cd fish-speech

conda create -n fish-speech python=3.10
conda activate fish-speech

pip3 install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1

apt install libsox-dev ffmpeg 

apt install build-essential \
    cmake \
    libasound-dev \
    portaudio19-dev \
    libportaudio2 \
    libportaudiocpp0

pip3 install -e .
</code></pre><p>Download <a href="https://speech.fish.audio/inference/">required models</a></p>
<pre tabindex="0"><code>huggingface-cli download fishaudio/fish-speech-1.4 --local-dir checkpoints/fish-speech-1.4
</code></pre><p>Prepare dataset</p>
<pre tabindex="0"><code>mkdir data
cp -r /home/username/GPT-SoVITS/output/slicer_opt data/
python tools/whisper_asr.py --audio-dir data/slicer_opt --save-dir data/slicer_opt --compute-type float32

python tools/vqgan/extract_vq.py data \
    --num-workers 1 --batch-size 16 \
    --config-name &#34;firefly_gan_vq&#34; \
    --checkpoint-path &#34;checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth&#34;
	
python tools/llama/build_dataset.py \
    --input &#34;data&#34; \
    --output &#34;data/protos&#34; \
    --text-extension .lab \
    --num-workers 16
</code></pre><p>Edit parameters by <code>nano fish_speech/configs/text2semantic_finetune.yaml</code> and start training</p>
<pre tabindex="0"><code>python fish_speech/train.py --config-name text2semantic_finetune \
    project=$project \
    +lora@model.model.lora_config=r_8_alpha_16
</code></pre><p>Note:</p>
<ul>
<li>The default numbers are too high for my setup, so both <code>num_workers</code> and <code>batch_size</code> needs to be lowered <a href="https://lightning.ai/docs/pytorch/stable/advanced/speed.html#num-workers">according to CPU cores and VRAM</a>.</li>
<li>For the first run, I set <code>max_steps: 10000</code> and <code>val_check_interval: 1000</code> to have 5 models that have lower steps with some diversity .</li>
<li>Things like <code>lr</code>, <code>weight_decay</code> and <code>num_warmup_steps</code> can be further adjusted accroding to <a href="https://modal.com/blog/fine-tuning-llms-hyperparameters-glossary-article">this article</a>. My setting is <code>lr: 1e-5</code>, <code>weight_decay: 1e-6</code>, <code>num_warmup_steps: 500</code>.</li>
<li>To check to training metrics such as loss curve, run <code>tensorboard --logdir fish-speech/results/tensorboard/version_xx/</code> and access <code>localhost:6006</code> via browser. Determine overfitting with the graph AND actuall hear to the inference result for each checkpoint.</li>
<li>At first I found out overfitting starts around 5000 step. Then a a second training for 5000 steps and find the best result is <code>step_000004000.ckpt</code>.</li>
<li>Training requires a newer GPU with <a href="https://github.com/deepspeedai/DeepSpeed/issues/6723">bf16</a> and no workaround so far.</li>
<li>When training a model for inferencing on an older GPU, use <code>precision: 32-true</code> in <code>fish_speech/configs/text2semantic_finetune.yaml</code> and <code>result += (self.lora_dropout(x).to(torch.float32) @ self.lora_A.to(torch.float32).transpose(0, 1) @ self.lora_B.to(torch.float32).transpose(0, 1)) * self.scaling.to(torch.float32)</code> in<code>/home/username/miniconda3/envs/fish-speech/lib/python3.10/site-packages/loralib/layers.py</code>.</li>
</ul>
<p>Training would take many hours on weak GPU. After finished, convert the LoRA weights</p>
<pre tabindex="0"><code>python tools/llama/merge_lora.py \
    --lora-config r_8_alpha_16 \
    --base-weight checkpoints/fish-speech-1.4 \
    --lora-weight results/$project/checkpoints/step_000005000.ckpt \
    --output checkpoints/fish-speech-1.4-yth-lora/
</code></pre><p>Generate prompt and semantic tokens</p>
<pre tabindex="0"><code>python tools/vqgan/inference.py \
    -i &#34;1.wav&#34; \
    --checkpoint-path &#34;checkpoints/fish-speech-1.4/firefly-gan-vq-fsq-8x1024-21hz-generator.pth&#34;
</code></pre><p><a href="https://github.com/jianchang512/stt/issues/51">Troubleshooting for old GPU 1</a> <code>Unable to load any of {libcudnn_ops.so.9.1.0, libcudnn_ops.so.9.1, libcudnn_ops.so.9, libcudnn_ops.so}</code></p>
<pre tabindex="0"><code>pip uninstall -y ctranslate2
pip install ctranslate2==3.24.0
</code></pre><p><a href="https://github.com/pytorch/pytorch/issues/143377">Troubleshooting for old GPU 2</a> <code>ImportError: cannot import name 'is_callable_allowed' from partially initialized module 'torch._dynamo.trace_rules'</code></p>
<pre tabindex="0"><code>conda install pytorch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1  pytorch-cuda=11.8 -c pytorch -c nvidia
</code></pre><p>Make it accessible from LAN <code>nano tools/run_webui.py</code></p>
<pre tabindex="0"><code>app.launch(server_name=&#34;0.0.0.0&#34;, server_port=7860, show_api=True)
</code></pre><p>Change the <code>--llama-checkpoint-path</code> to the newly trained LoRA, and start WebUI (added <code>--half</code> for my old GPU to avoid bf16 error)</p>
<pre tabindex="0"><code>GRADIO_SHARE=0 GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=7860 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python -m tools.webui \
    --llama-checkpoint-path &#34;checkpoints/fish-speech-1.4-yth-lora&#34; \
    --decoder-checkpoint-path &#34;checkpoints/fish-speech-1.4/firefly-gan-vq-fsq-8x1024-21hz-generator.pth&#34; \
    --decoder-config-name firefly_gan_vq \
    --half
</code></pre><p>Parameters for inferencing
Enable Reference Audio
check Text Normalization
Interative Prompt Lenth 200
Top-P 0.8
Temperature 0.7
Repetition Penalty 1.5
Set Seed</p>
<p>Note:</p>
<ul>
<li>higher number to compensate overfitted model, lower number for underfitted model.</li>
<li>certain punctuation or tab space may trigger noise generation. Text normalization suppose to address these issue but sometimes I still need to find &amp; replace.</li>
</ul>
<p>However, a bug <code>Negative code found</code> occurs quite frequent while inferencing without solution by now. Give up.</p>
<h3 id="cosyvoice">CosyVoice<a hidden class="anchor" aria-hidden="true" href="#cosyvoice">#</a></h3>
<p><a href="%5Bhttps://github.com/FunAudioLLM/CosyVoice">CosyVoice</a> is one from the FunAudioLLM toolkits, which developed by the same team from Alibaba&rsquo;s Qwen I use a lot.</p>
<p>Install</p>
<pre tabindex="0"><code>git clone --recursive https://github.com/FunAudioLLM/CosyVoice.git &amp;&amp; cd CosyVoice
git submodule update --init --recursive
conda create -n cosyvoice -y python=3.10
conda activate cosyvoice
conda install -y -c conda-forge pynini==2.1.5
sudo apt-get install sox libsox-dev -y
pip install -r requirements.txt
</code></pre><p>Download Pretrained Models</p>
<pre tabindex="0"><code>git lfs install
mkdir -p pretrained_models
git clone https://huggingface.co/FunAudioLLM/CosyVoice2-0.5B pretrained_models/CosyVoice2-0.5B
git clone https://huggingface.co/FunAudioLLM/CosyVoice-300M pretrained_models/CosyVoice-300M
git clone https://huggingface.co/FunAudioLLM/CosyVoice-300M-SFT pretrained_models/CosyVoice-300M-SFT
git clone https://huggingface.co/FunAudioLLM/CosyVoice-300M-Instruct pretrained_models/CosyVoice-300M-Instruct
</code></pre><p>Run with</p>
<pre tabindex="0"><code>GRADIO_SHARE=0 GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=7860 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python -m  webui.py --port 50000 --model_dir pretrained_models/CosyVoice-300M
</code></pre><p><a href="https://github.com/huggingface/lerobot/issues/742">Troubleshooting &ldquo;GLIBCXX_3.4.29&rsquo; not found&rdquo;</a> with <a href="https://stackoverflow.com/questions/48453497/anaconda-libstdc-so-6-version-glibcxx-3-4-20-not-found#73101774">this</a></p>
<pre tabindex="0"><code>strings /usr/lib/x86_64-linux-gnu/libstdc++.so.6 | grep GLIBCXX
strings $CONDA_PREFIX/lib/libstdc++.so.6 | grep GLIBCXX

nano ~/.bashrc
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH

find / -name &#34;libstdc++.so*&#34;
rm /home/username/anaconda3/lib/python3.11/site-packages/../../libstdc++.so.6
ln -s /home/username/text-generation-webui/installer_files/env/lib/libstdc++.so.6.0.29 /home/username/anaconda3/lib/python3.11/site-packages/../../libstdc++.so.6
</code></pre><p>It ends up working fine but not as good as GPT-SoVITS. Hope their 3.0 version can pump it up.</p>
<h2 id="voice-conversion">Voice Conversion<a hidden class="anchor" aria-hidden="true" href="#voice-conversion">#</a></h2>
<p>Both <a href="https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI">RVC</a> and <a href="https://github.com/Plachtaa/seed-vc">Seed-VC</a> are intended to replace my good old <a href="https://github.com/svc-develop-team/so-vits-svc">so-vits-svc</a> instance.</p>
<h3 id="retrieval-based-voice-conversion">Retrieval-based-Voice-Conversion<a hidden class="anchor" aria-hidden="true" href="#retrieval-based-voice-conversion">#</a></h3>
<p>Install</p>
<pre tabindex="0"><code>git clone https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI &amp;&amp; cd Retrieval-based-Voice-Conversion-WebUI
conda create -n rvc -y python=3.8
conda activate rvc
pip install torch torchvision torchaudio
pip install pip==24.0
pip install -r requirements.txt
python tools/download_models.py
sudo apt install ffmpeg
wget https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/rmvpe.pt
</code></pre><p>Run with <code>python infer-web.py</code>, fill up following then click buttons step by step with default settings:</p>
<pre tabindex="0"><code>Enter the experiment name:/path/to/raw/
</code></pre><p><a href="https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/issues/2423">Troubleshooting &ldquo;enabled=hps.train.fp16_run&rdquo;</a></p>
<h3 id="seed-vc">Seed-VC<a hidden class="anchor" aria-hidden="true" href="#seed-vc">#</a></h3>
<p>Install</p>
<pre tabindex="0"><code>git clone https://github.com/Plachtaa/seed-vc &amp;&amp; cd Retrieval-based-Voice-Conversion-WebUI
conda create -n seedvc -y python=3.10
conda activate seedvc
pip install -r requirements.txt
GRADIO_SHARE=0 GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=7860 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python app.py --enable-v1 --enable-v2
</code></pre><p>Settings</p>
<pre tabindex="0"><code>#V2
Diffusion Steps: 100
Length Adjust: 1
Intelligibility CFG Rate: 0
Similarity CFG Rate: 1
Top-p: 1
Temperature: 1
Repetition Penalty: 2
convert style/emotion/accent: check

#V1
Diffusion Steps: 100
Length Adjust: 1
Inference CFG Rate: 1
Use F0 conditioned model: check
Auto F0 adjust: check
Pitch shift: 0
</code></pre><p>Training</p>
<pre tabindex="0"><code>python train.py --config /home/username/seed-vc/configs/presets/config_dit_mel_seed_uvit_whisper_base_f0_44k.yml --dataset-dir /home/username/GPT-SoVITS-v4/output/slicer_opt --run-name username --batch-size 6 --max-steps 10000 --max-epochs 10000 --save-every 1000 --num-workers 1

accelerate launch train_v2.py --dataset-dir /home/username/GPT-SoVITS-v4/output/slicer_opt --run-name username-v2 --batch-size 6 --max-steps 2000 --max-epochs 2000 --save-every 200 --num-workers 0 --train-cfm
</code></pre><p>Using checkpoints</p>
<pre tabindex="0"><code>#Voice Conversion Web UI
GRADIO_SHARE=0 GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=7860 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python app_vc.py --checkpoint ./runs/test01/ft_model.pth --config ./configs/presets/config_dit_mel_seed_uvit_whisper_base_f0_44k.yml --fp16 False

#Singing Voice Conversion Web UI
GRADIO_SHARE=0 GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=7860 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python app_svc.py --checkpoint ./runs/username/DiT_epoch_00029_step_08000.pth --config ./configs/presets/config_dit_mel_seed_uvit_whisper_base_f0_44k.yml --fp16 False

#V2 model Web UI
GRADIO_SHARE=0 GRADIO_SERVER_NAME=0.0.0.0 GRADIO_SERVER_PORT=7860 GRADIO_ANALYTICS_ENABLED=0 DISABLE_TELEMETRY=1 DO_NOT_TRACK=1 python app_vc_v2.py --cfm-checkpoint-path runs/Satine-V2/CFM_epoch_00000_step_00600.pth
</code></pre><p>It turned out V1 model with Singing Voice Conversion Web UI <code>app_svc.py</code> performs the best.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://techshinobi.org/tags/cosyvoice/">CosyVoice</a></li>
      <li><a href="https://techshinobi.org/tags/gpt-sovits/">GPT-SoVITS</a></li>
      <li><a href="https://techshinobi.org/tags/fish-speech/">Fish-Speech</a></li>
      <li><a href="https://techshinobi.org/tags/opensource/">Opensource</a></li>
      <li><a href="https://techshinobi.org/tags/retrieval-based-voice-conversion/">Retrieval-Based-Voice-Conversion</a></li>
      <li><a href="https://techshinobi.org/tags/rvc/">RVC</a></li>
      <li><a href="https://techshinobi.org/tags/seed-vc/">Seed-VC</a></li>
      <li><a href="https://techshinobi.org/tags/voice-clone/">Voice Clone</a></li>
      <li><a href="https://techshinobi.org/tags/model-training/">Model Training</a></li>
      <li><a href="https://techshinobi.org/tags/inference/">Inference</a></li>
      <li><a href="https://techshinobi.org/tags/realtime-voice-changing/">Realtime Voice Changing</a></li>
      <li><a href="https://techshinobi.org/tags/diffusion/">Diffusion</a></li>
      <li><a href="https://techshinobi.org/tags/text-to-speech/">Text-to-Speech</a></li>
      <li><a href="https://techshinobi.org/tags/whisper/">Whisper</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://techshinobi.org/posts/learn-ai/">
    <span class="title">« Prev</span>
    <br>
    <span>My AI Learning Materials and News Feeds (Updated)</span>
  </a>
  <a class="next" href="https://techshinobi.org/posts/harbor-wsl/">
    <span class="title">Next »</span>
    <br>
    <span>Migrating Harbor instance from Linux to WSL2</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Revisiting Voice Cloning with GPT-SoVITS and so on on x"
            href="https://x.com/intent/tweet/?text=Revisiting%20Voice%20Cloning%20with%20GPT-SoVITS%20and%20so%20on&amp;url=https%3a%2f%2ftechshinobi.org%2fposts%2fvits-re%2f&amp;hashtags=CosyVoice%2cGPT-SoVITS%2cfish-speech%2cOpensource%2cRetrieval-based-Voice-Conversion%2cRVC%2cSeed-VC%2cVoiceClone%2cModelTraining%2cInference%2crealtimevoicechanging%2cDiffusion%2cText-to-Speech%2cwhisper">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Revisiting Voice Cloning with GPT-SoVITS and so on on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftechshinobi.org%2fposts%2fvits-re%2f&amp;title=Revisiting%20Voice%20Cloning%20with%20GPT-SoVITS%20and%20so%20on&amp;summary=Revisiting%20Voice%20Cloning%20with%20GPT-SoVITS%20and%20so%20on&amp;source=https%3a%2f%2ftechshinobi.org%2fposts%2fvits-re%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Revisiting Voice Cloning with GPT-SoVITS and so on on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2ftechshinobi.org%2fposts%2fvits-re%2f&title=Revisiting%20Voice%20Cloning%20with%20GPT-SoVITS%20and%20so%20on">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Revisiting Voice Cloning with GPT-SoVITS and so on on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftechshinobi.org%2fposts%2fvits-re%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Revisiting Voice Cloning with GPT-SoVITS and so on on whatsapp"
            href="https://api.whatsapp.com/send?text=Revisiting%20Voice%20Cloning%20with%20GPT-SoVITS%20and%20so%20on%20-%20https%3a%2f%2ftechshinobi.org%2fposts%2fvits-re%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Revisiting Voice Cloning with GPT-SoVITS and so on on telegram"
            href="https://telegram.me/share/url?text=Revisiting%20Voice%20Cloning%20with%20GPT-SoVITS%20and%20so%20on&amp;url=https%3a%2f%2ftechshinobi.org%2fposts%2fvits-re%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Revisiting Voice Cloning with GPT-SoVITS and so on on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Revisiting%20Voice%20Cloning%20with%20GPT-SoVITS%20and%20so%20on&u=https%3a%2f%2ftechshinobi.org%2fposts%2fvits-re%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://techshinobi.org/">Tech Shinobi</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
